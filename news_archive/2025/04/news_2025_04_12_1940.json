[
    {
        "Title": "Leak confirms OpenAI's GPT 4.1 is coming before GPT 5.0",
        "Link": "https://www.bleepingcomputer.com/news/artificial-intelligence/leak-confirms-openais-gpt-41-is-coming-before-gpt-50/",
        "Summary": "OpenAI is working on yet another AI model reportedly called GPT-4.1, a successor to GPT-4o, which is expected to come before GPT 5.0 [...]",
        "Date": "2025-04-12",
        "Content": "OpenAI is working on yet another AI model reportedly called GPT-4.1, a successor to GPT-4o, which is expected to come before GPT 5.0\nThe Verge\nrecently reported that OpenAI plans to launch GPT-4.1, which is an upgrade to the existing GPT-4o. And now, we have more reasons to believe the model is indeed coming.\nAs\nspotted\nby AI researcher Tibor Blaho, OpenAI is already\ntesting\nmodel art for o3, o4-mini, and GPT-4.1 (including nano and mini variants) on the OpenAI API platform.\nGPT 4.1 references on OpenAI's website\nThis confirms that GPT-4.1 does exist, but it doesn't appear to be a successor to GPT-4.5.\nMy understanding is that GPT-4.1 is a successor to GPT-4o, which is multimodal. On the other hand, GPT-4.5 focuses more on creativity and delivering better answers.\nIn a \"\nPre-Training GPT-4.5\n\" video from OpenAI, founder and CEO Sam Altman dropped hints that OpenAI has a team that wants to redo GPT-4 from scratch using new training data and systems.\n\"If you guys could go pick whoever you wanted, what is the smallest team from OpenAI that could go retrain GPT-4 from scratch today with everything we know and have and all the systems work?\" Sam said.\nIt's unclear if Altman is referring to the new GPT-4.1 model, but if I were to bet, I'd bet on GPT-4.1.\nAlso, GPT-5 isn't happening anytime soon, as OpenAI plans to focus on\no3, o4-mini, o4-mini-high\n, and GPT-4.1 (including nano and mini variants).\nTop 10 MITRE ATT&CK\n©\nTechniques Behind 93% of Attacks\nBased on an analysis of 14M malicious actions, discover the top 10 MITRE ATT&CK techniques behind 93% of attacks and how to defend against them.\nRead the Red Report 2025\nRelated Articles:\nChatGPT's o4-mini, o4-mini-high and o3 spotted ahead of release\nOpenAI's GPT 4.5 spotted in Android beta, launch imminent\nOpenAI wants ChatGPT to 'know you over your life' with new Memory update\nClaude copies ChatGPT with $200 Max plan, but users aren't happy\nOpenAI tests watermarking for ChatGPT-4o Image Generation model"
    },
    {
        "Title": "Tycoon2FA phishing kit targets Microsoft 365 with new tricks",
        "Link": "https://www.bleepingcomputer.com/news/security/tycoon2fa-phishing-kit-targets-microsoft-365-with-new-tricks/",
        "Summary": "Phishing-as-a-service (PhaaS) platform Tycoon2FA, known for bypassing multi-factor authentication on Microsoft 365 and Gmail accounts, has received updates that improve its stealth and evasion capabilities. [...]",
        "Date": "2025-04-12",
        "Content": "Phishing-as-a-service (PhaaS) platform Tycoon2FA, known for bypassing multi-factor authentication on Microsoft 365 and Gmail accounts, has received updates that improve its stealth and evasion capabilities.\nTycoon2FA was discovered in October 2023 by Sekoia researchers, who later\nreported significant updates\non the phishing kit that increased its sophistication and effectiveness.\nTrustwave now reports\nthat the Tycoon 2FA threat actors have added several improvements that bolster the kit's ability to bypass detection and endpoint security protections.\nThe first highlighted change is the\nuse of invisible Unicode characters\nto hide binary data within JavaScript, as first reported by Juniper Threat Labs in February. This tactic allows the payload to be decoded and executed as normal at runtime while evading manual (human) and static pattern-matching analysis.\nUsing Unicode to hide malicious code snippets\nSource: Trustwave\nThe second development is the switch from Cloudflare Turnstile to a self-hosted CAPTCHA rendered via HTML5 canvas with randomized elements.\nLikely, the creators of Tycoon 2FA opted for this change to evade fingerprinting and flagging by domain reputation systems and gain better customization control over the page's content.\nThe third major change is the inclusion of anti-debugging JavaScript that detects browser automation tools like PhantomJS and Burp Suite and blocks certain actions associated with analysis.\nWhen suspicious activity is detected or the CAPTCHA fails (potential indication of security bots), the user is served a decoy page or is redirected to a legitimate website like rakuten.com.\nThe kit's new anti-debug logic\nSource: Trustwave\nTrustwave underlines that while these evasion techniques aren't novel individually, they make a big difference when combined, complicating detection and analysis that can uncover phishing infrastructure and lead to takedowns and disruption.\nSVG lures surging\nIn a separate but related report, Trustwave says it has identified a\ndramatic increase\nin phishing attacks using\nmalicious SVG (Scalable Vector Graphics) files\n, driven by PhaaS platforms like Tycoon2FA, Mamba2FA, and Sneaky2FA.\nThe cybersecurity firm reports a steep rise of 1,800% from April 2024 to March 2025, indicating a clear shift in tactics favoring the particular file format.\nSVG file attachments used in phishing attacks\nSource: Trustwave\nThe Malicious SVGs used in the phishing attacks are for images disguised as voice messages, logos, or cloud document icons. However, SVG files can also contain JavaScript, which is automatically triggered when the image is rendered in browsers.\nThis code is obfuscated using base64 encoding, ROT13, XOR encryption, and junk code, so detection is less likely.\nThe function of the malicious code is to redirect the message recipients to Microsoft 365 phishing pages that steal their account credentials.\nA case study presented in the Trustwave report concerns a fake Microsoft Teams voicemail alert with an SVG file attachment disguised as an audio message. Clicking it opens an external browser that executes JavaScript, redirecting to a fake Office 365 login page.\nMicrosoft Teams lure\nSource: Trustwave\nThe rise of PhaaS platforms and SVG-based phishing calls for heightened vigilance and the need for sender authenticity verification.\nAn effective defense measure is to block or flag SVG attachments in email gateways and use phishing-resistant MFA methods like FIDO-2 devices.\nTop 10 MITRE ATT&CK\n©\nTechniques Behind 93% of Attacks\nBased on an analysis of 14M malicious actions, discover the top 10 MITRE ATT&CK techniques behind 93% of attacks and how to defend against them.\nRead the Red Report 2025\nRelated Articles:\nPhishing platform 'Lucid' behind wave of iOS, Android SMS attacks\nPhishing-as-a-service operation uses DNS-over-HTTPS for evasion\nMalicious Adobe, DocuSign OAuth apps target Microsoft 365 accounts\nNew Darcula phishing service targets iPhone users via iMessage\nDarcula PhaaS can now auto-generate phishing kits for any brand"
    },
    {
        "Title": "AI-hallucinated code dependencies become new supply chain risk",
        "Link": "https://www.bleepingcomputer.com/news/security/ai-hallucinated-code-dependencies-become-new-supply-chain-risk/",
        "Summary": "A new class of supply chain attacks named 'slopsquatting' has emerged from the increased use of generative AI tools for coding and the model's tendency to \"hallucinate\" non-existent package names. [...]",
        "Date": "2025-04-12",
        "Content": "A new class of supply chain attacks named 'slopsquatting' has emerged from the increased use of generative AI tools for coding and the model's tendency to \"hallucinate\" non-existent package names.\nThe term slopsquatting was coined by security researcher\nSeth Larson\nas a spin on typosquatting, an attack method that tricks developers into installing malicious packages by using names that closely resemble popular libraries.\nUnlike typosquatting, slopsquatting doesn't rely on misspellings. Instead, threat actors could create malicious packages on indexes like PyPI and npm named after ones commonly made up by AI models in coding examples.\nA\nresearch paper\nabout package hallucinations published in March 2025 demonstrates that in roughly 20% of the examined cases (576,000 generated Python and JavaScript code samples), recommended packages didn't exist.\nThe situation is worse on open-source LLMs like CodeLlama, DeepSeek, WizardCoder, and Mistral, but commercial tools like ChatGPT-4 still hallucinated at a rate of about 5%, which is significant.\nHallucination rates for various LLMs\nSource: arxiv.org\nWhile the number of unique hallucinated package names logged in the study was large, surpassing 200,000, 43% of those were consistently repeated across similar prompts, and 58% re-appeared at least once again within ten runs.\nThe study showed that 38% of these hallucinated package names appeared inspired by real packages, 13% were the results of typos, and the remainder, 51%, were completely fabricated.\nAlthough there are no signs that attackers have started taking advantage of this new type of attack, researchers from open-source cybersecurity company Socket warn that hallucinated package names are common, repeatable, and semantically plausible, creating a predictable attack surface that could be easily weaponized.\n\"Overall, 58% of hallucinated packages were repeated more than once across ten runs, indicating that a majority of hallucinations are not just random noise, but repeatable artifacts of how the models respond to certain prompts,\"\nexplains the Socket researchers\n.\n\"That repeatability increases their value to attackers, making it easier to identify viable slopsquatting targets by observing just a small number of model outputs.\"\nOverview of the supply chain risk\nSource: arxiv.org\nThe only way to mitigate this risk is to verify package names manually and never assume a package mentioned in an AI-generated code snippet is real or safe.\nUsing dependency scanners, lockfiles, and hash verification to pin packages to known, trusted versions is an effective way to improve security\nThe research has shown that lowering AI \"temperature\" settings (less randomness) reduces hallucinations, so if you're into AI-assisted or vibe coding, this is an important factor to consider.\nUltimately, it is prudent to always test AI-generated code in a safe, isolated environment before running or deploying it in production environments.\nTop 10 MITRE ATT&CK\n©\nTechniques Behind 93% of Attacks\nBased on an analysis of 14M malicious actions, discover the top 10 MITRE ATT&CK techniques behind 93% of attacks and how to defend against them.\nRead the Red Report 2025\nRelated Articles:\nInfostealer campaign compromises 10 npm packages, targets devs\nNorth Korean Lazarus hackers infect hundreds via npm packages\nGoogle takes on Cursor with Firebase Studio, its AI builder for vibe coding\nOpenAI's $20 ChatGPT Plus is now free for students until the end of May\nRecent GitHub supply chain attack traced to leaked SpotBugs token"
    }
]