[
    {
        "Title": "Google Fixed Cloud Run Vulnerability Allowing Unauthorized Image Access via IAM Misuse",
        "Link": "https://thehackernews.com/2025/04/google-fixed-cloud-run-vulnerability.html",
        "Summary": "Cybersecurity researchers have disclosed details of a now-patched privilege escalation vulnerability in Google Cloud Platform (GCP) Cloud Run that could have allowed a malicious actor to access container images and even inject malicious code.\n\"The vulnerability could have allowed such an identity to abuse its Google Cloud Run revision edit permissions in order to pull private Google Artifact",
        "Date": "2025-04-02",
        "Content": "Cybersecurity researchers have disclosed details of a now-patched privilege escalation vulnerability in Google Cloud Platform (GCP) Cloud Run that could have allowed a malicious actor to access container images and even inject malicious code.\n\"The vulnerability could have allowed such an identity to abuse its Google Cloud Run revision edit permissions in order to pull private Google Artifact Registry and Google Container Registry images in the same account,\" Tenable security researcher Liv Matan\nsaid\nin a report shared with The Hacker News.\nThe security shortcoming has been codenamed ImageRunner by the cybersecurity company. Following responsible disclosure, Google addressed the problem as of January 28, 2025.\nGoogle Cloud Run\nis a fully managed service for executing containerized applications in a scalable, serverless environment. When the technology is used to run a service, container images are retrieved from the\nArtifact Registry\n(or Docker Hub) for subsequent deployment by specifying the image URL.\nAt issue is the fact that there are certain identities that lack container registry permissions but that have edit permissions on Google Cloud Run revisions.\nEach time a Cloud Run service is deployed or updated, a new version is created. And each time a Cloud Run revision is deployed, a\nservice agent account\nis used to pull the necessary images.\n\"If an attacker gains certain permissions within a victim's project -- specifically run.services.update and iam.serviceAccounts.actAs permissions -- they could modify a Cloud Run service and deploy a new revision,\" Matan explained. \"In doing so, they could specify any private container image within the same project for the service to pull.\"\nWhat's more, the attacker could access sensitive or proprietary images stored in a victim's registries and even introduce malicious instructions that, when executed, could be abused to extract secrets, exfiltrate sensitive data, or even open a reverse shell to a machine under their control.\nThe patch released by Google now ensures that the user or service account creating or updating a Cloud Run resource has explicit permission to access the container images.\n\"The principal (user or service account) creating or updating a Cloud Run resource now needs explicit permission to access the container image(s),\" the tech giant\nsaid\nin its release notes for Cloud Run in January 2025.\n\"When using Artifact Registry, ensure the principal has the Artifact Registry Reader (roles/artifactregistry.reader) IAM role on the project or repository containing the container image(s) to deploy.\"\nTenable has characterized ImageRunner as an instance of what it calls Jenga, which arises due to the interconnected nature of various cloud services, causing security risks to be passed along.\n\"Cloud providers build their services on top of their other existing services,\" Matan said. \"If one service gets attacked or is compromised, the other ones built on top of it inherit the risk and become vulnerable as well.\"\n\"This scenario opens the door for attackers to discover novel privilege escalation opportunities and even vulnerabilities, and introduces new hidden risks for defenders.\"\nThe disclosure comes weeks after Praetorian detailed several ways a lower-privilege principal can abuse an Azure virtual machine (VM) to gain control over an Azure subscription -\nExecute commands on an Azure VM associated with an administrative managed identity\nLog in to an Azure VM associated with an administrative managed identity\nAttach an existing administrative user-assigned managed identity to an existing Azure VM and execute commands in that VM\nCreate a new Azure VM, attach an existing administrative managed identity to it, and execute commands in that VM by using data plane actions\n\"After obtaining the Owner role for a subscription, an attacker may be able to leverage their broad control over all subscription resources to find a privilege escalation path to the Entra ID tenant,\" security researchers Andrew Chang and Elgin Lee\nsaid\n.\n\"This path is predicated on a compute resource in the victim subscription with a service principal with Entra ID permissions that may allow it to escalate itself to Global Administrator.\"\nFound this article interesting?  Follow us on\nTwitter\n\nand\nLinkedIn\nto read more exclusive content we post."
    },
    {
        "Title": "Helping Your Clients Achieve NIST Compliance: A Step by Step Guide for Service Providers",
        "Link": "https://thehackernews.com/2025/04/helping-your-clients-achieve-nist.html",
        "Summary": "Introduction\nAs the cybersecurity landscape evolves, service providers play an increasingly vital role in safeguarding sensitive data and maintaining compliance with industry regulations. The National Institute of Standards and Technology (NIST) offers a comprehensive set of frameworks that provide a clear path to achieving robust cybersecurity practices.\nFor service providers, adhering to NIST",
        "Date": "2025-04-02",
        "Content": "Introduction\nAs the cybersecurity landscape evolves, service providers play an increasingly vital role in safeguarding sensitive data and maintaining compliance with industry regulations. The National Institute of Standards and Technology (NIST) offers a comprehensive set of frameworks that provide a clear path to achieving robust cybersecurity practices.\nFor service providers, adhering to NIST standards is a strategic business decision. Compliance not only protects client data but also enhances credibility, streamlines incident response, and provides a competitive edge.\nThe\nstep-by-step guide\nis designed to help service providers understand and implement NIST compliance for their clients. By following the guide, you will:\nUnderstand the importance of NIST compliance and how it impacts service providers.\nLearn about key NIST frameworks, including NIST Cybersecurity Framework (CSF 2.0), NIST 800-53, and NIST 800-171.\nFollow a structured compliance roadmap—from conducting a gap analysis to implementing security controls and monitoring risks.\nLearn how to overcome common compliance challenges using best practices and automation tools.\nEnsure long-term compliance and security maturity, strengthening trust with clients and enhancing market competitiveness.\nWhat is NIST Compliance and Why Does it Matter for Service Providers?\nNIST compliance involves aligning an organization's cybersecurity policies, processes, and controls with standards set by the National Institute of Standards and Technology. These standards help organizations manage cybersecurity risks effectively by providing a structured approach to data protection, risk assessment, and incident response.\nFor service providers, achieving NIST compliance means:\nEnhanced security:\nImproved ability to identify, assess, and mitigate cybersecurity risks.\nRegulatory compliance:\nAlignment with industry standards such as HIPAA, PCI-DSS, and CMMC.\nMarket differentiation:\nEstablishes trust with clients, positioning providers as reliable security partners.\nEfficient incident response:\nEnsures a structured process for managing security incidents.\nOperational efficiency:\nSimplifies compliance with clear frameworks and automation tools.\nWho Needs NIST Compliance?\nNIST compliance is essential for various industries, including:\nGovernment Contractors\n– Required for compliance with CMMC and NIST 800-171 to protect Controlled Unclassified Information (CUI).\nHealthcare Organizations\n– Supports HIPAA compliance and protects patient data.\nFinancial Services\n– Ensures data security and fraud prevention.\nManaged Service Providers (MSPs) and Managed Security Service Providers (MSSPs)\n– Helps secure client environments and meet contractual security requirements.\nTechnology & Cloud Service Providers\n– Enhances cloud security practices and aligns with federal cybersecurity initiatives.\nKey NIST Frameworks for Compliance\nNIST offers multiple cybersecurity frameworks, but the most relevant for service providers include:\nNIST Cybersecurity Framework (CSF 2.0)\n: A flexible, risk-based framework designed for businesses of all sizes and industries. It consists of six core functions—Identify, Protect, Detect, Respond, Recover, and Govern—to help organizations strengthen their security posture.\nNIST 800-53\n: A comprehensive set of security and privacy controls designed for federal agencies and contractors. Many private-sector organizations also adopt these controls to standardize cybersecurity measures.\nNIST 800-171\n: Focused on protecting Controlled Unclassified Information (CUI) in non-federal systems, particularly for companies that work with the Department of Defense (DoD) and other government agencies.\nCommon Challenges in Achieving NIST Compliance for Clients and How to Overcome Them\nHere are some common challenges service providers encounter when working to achieve NIST compliance and strategies to overcome them:\nIncomplete Asset Inventory:\nAn incomplete asset inventory is a common challenge due to the sheer number of assets organizations manage. To overcome this, many organizations rely on automated tools and routine audits to ensure all IT assets are accurately accounted for.\nLimited Budgets:\nLimited budgets are a frequent obstacle for many organizations, making it essential to focus on high-impact controls, leverage open-source tools, and automate compliance tasks to manage costs effectively.\nThird-Party Risks:\nThird-party risks pose significant challenges for organizations that rely on external vendors. To address this, many organizations conduct vendor assessments, include NIST-aligned clauses in contracts, and perform regular audits to ensure compliance.\nAddressing these challenges proactively helps streamline compliance, enhance security, and reduce risks.\nStep-by-Step Guide to Achieving NIST Compliance\nAs mentioned above, achieving NIST compliance for clients presents numerous challenges for service providers, making the process complex and daunting. In fact,\n93% of service providers struggle to navigate cybersecurity frameworks like NIST\nor ISO, and a staggering 98% report feeling overwhelmed by compliance requirements, with only 2% expressing confidence in their approach.\nHowever, by adopting a step-by-step method, service providers can simplify the process, making compliance more manageable and accessible for MSPs and MSSPs.\nThe main steps for achieving NIST Compliance are:\nConduct a Gap Analysis\nDevelop Security Policies and Procedures\nConduct a Comprehensive Risk Assessment\nImplement Security Controls\nDocument Compliance Efforts\nConduct Regular Audits and Assessments\nContinuous Monitoring and Improvement\nExplore our\ncomprehensive guide\nfor a detailed approach to achieving NIST compliance.\nThe Role of Automation in NIST Compliance\nAligning with NIST guidelines enables MSPs and MSSPs to operate more efficiently by providing a clear and standardized framework, eliminating the need to create new processes for each client. Integrating automation tools like Cynomi's platform further enhances efficiency by streamlining risk assessments, monitoring security controls, and generating compliance reports with minimal manual effort.\nThis approach saves time by automating risk assessments and compliance documentation, improves accuracy by reducing human error in compliance tracking, and simplifies audits with pre-built reports and templates. Cynomi's platform is particularly effective, automating risk identification, scoring, and compliance documentation while reducing manual work by up to 70%.\nConclusion\nAchieving NIST compliance is a vital step for service providers aiming to protect client data, enhance security posture, and build lasting trust. A structured approach - combined with automated tools - makes it easier to manage compliance efficiently and proactively. By adopting NIST frameworks, service providers can not only meet regulatory requirements but also gain a competitive advantage in the cybersecurity market.\nFor a detailed look at how to achieve NIST compliance, explore our comprehensive\nguide here.\nFound this article interesting?\nThis article is a contributed piece from one of our valued partners.\nFollow us on\nTwitter\n\nand\nLinkedIn\nto read more exclusive content we post."
    },
    {
        "Title": "Outlaw Group Uses SSH Brute-Force to Deploy Cryptojacking Malware on Linux Servers",
        "Link": "https://thehackernews.com/2025/04/outlaw-group-uses-ssh-brute-force-to.html",
        "Summary": "Cybersecurity researchers have shed light on an \"auto-propagating\" cryptocurrency mining botnet called Outlaw (aka Dota) that's known for targeting SSH servers with weak credentials.\n\"Outlaw is a Linux malware that relies on SSH brute-force attacks, cryptocurrency mining, and worm-like propagation to infect and maintain control over systems,\" Elastic Security Labs said in a new analysis",
        "Date": "2025-04-02",
        "Content": "Cybersecurity researchers have shed light on an \"auto-propagating\" cryptocurrency mining botnet called\nOutlaw\n(aka Dota) that's known for targeting SSH servers with weak credentials.\n\"Outlaw is a Linux malware that relies on SSH brute-force attacks, cryptocurrency mining, and worm-like propagation to infect and maintain control over systems,\" Elastic Security Labs\nsaid\nin a new analysis published Tuesday.\nOutlaw is also the name given to the threat actors behind the malware. It's believed to be of Romanian origin. Other hacking groups\ndominating\nthe cryptojacking landscape include 8220, Keksec (aka Kek Security), Kinsing, and TeamTNT.\nActive\nsince at least late 2018\n, the\nhacking crew\nhas\nbrute-forced SSH servers\n, abusing the foothold to conduct reconnaissance and maintain persistence on the compromised hosts by adding their own SSH keys to the \"authorized_keys\" file.\nThe\nattackers\nare also known to incorporate a multi-stage infection process that involves using a dropper shell script (\"tddwrt7s.sh​\") to download an archive file (\"dota3.tar.gz\"), which is then unpacked to launch the miner while also taking steps to remove traces of past compromises and\nkill both the competition and their own previous miners\n.\nA\nnotable feature\nof the malware is an initial access component (aka BLITZ) that allows for self-propagation of the malware in a botnet-like fashion by scanning for vulnerable systems running an SSH service. The brute-force module is configured to fetch a target list from an SSH command-and-control (C2) server to further perpetuate the cycle.\nSome iterations of the attacks have also\nresorted\nto exploiting Linux- and Unix-based operating systems susceptible to\nCVE-2016-8655\nand\nCVE-2016-5195\n(aka\nDirty COW\n), as well as attack systems with weak Telnet credentials. Upon gaining initial access, the malware deploys\nSHELLBOT\nfor remote control via a C2 server using an IRC channel.\nSHELLBOT, for its part, enables the execution of arbitrary shell commands, downloads and runs additional payloads, launches DDoS attacks, steals credentials, and exfiltrates sensitive information.\nAs part of its mining process, it determines the CPU of the infected system and enables hugepages for all CPU cores to increase memory access efficiency. The malware also makes use of a binary called kswap01 to ensure persistent communications with the threat actor's infrastructure.\n\"Outlaw remains active despite using basic techniques like SSH brute-forcing, SSH key manipulation, and cron-based persistence,\" Elastic said. \"The malware deploys modified XMRig miners, leverages IRC for C2, and includes publicly available scripts for persistence and defense evasion.\"\nFound this article interesting?  Follow us on\nTwitter\n\nand\nLinkedIn\nto read more exclusive content we post."
    },
    {
        "Title": "How SSL Misconfigurations Impact Your Attack Surface",
        "Link": "https://thehackernews.com/2025/04/how-ssl-misconfigurations-impact-your.html",
        "Summary": "When assessing an organization’s external attack surface, encryption-related issues (especially SSL misconfigurations) receive special attention. Why? Their widespread use, configuration complexity, and visibility to attackers as well as users make them more likely to be exploited.&nbsp;\nThis highlights how important your SSL configurations are in maintaining your web application security and",
        "Date": "2025-04-02",
        "Content": "When assessing an organization's external attack surface, encryption-related issues (especially SSL misconfigurations) receive\nspecial attention\n. Why? Their widespread use, configuration complexity, and visibility to attackers as well as users make them more likely to be exploited.\nThis highlights how important your SSL configurations are in maintaining your web application security and minimizing your attack surface. However, research shows that most (53.5%) websites have\ninadequate security\nand that\nweak SSL/TLS configuration\nis amongst the most common application vulnerabilities.\nGet your SSL configuration right, and you'll enhance your cyber resilience and keep your apps and data safe. Get it wrong, however, and you can increase your organization's attack surface, exposing your business to more cyberattacks. We'll explore the impacts of SSL misconfigurations and explain why they present such a significant attack surface risk. Then, we'll show you how a solid\nEASM platform\ncan help overcome the challenges associated with detecting misconfiguration issues.\nUnderstanding SSL misconfigurations and attack surface\nAn SSL misconfiguration occurs when SSL certificates are improperly set up or managed, leading to vulnerabilities within an organization's network. These misconfigurations can include outdated encryption algorithms, incorrect certificate setup, expired SSL certificates, and more. Such vulnerabilities directly affect an organization's attack surface by creating possible entry routes for hackers.\nSSL misconfiguration: A significant attack surface risk\nSSL certificates provide a secure channel for data transmission between clients and servers. They authenticate websites' identities, ensuring users communicate with the intended entity. Misconfigured SSL certificates, however, can lead to risks, such as:\nMan-in-the-middle (MITM) attacks:\nMITM attacks occur when an attacker intercepts communication between two parties — typically a user and a web service — without their knowledge, allowing the attacker to eavesdrop on, modify, or redirect the communication. SSL stripping and certificate impersonation can both lead to MITM attacks.\nEavesdropping:\nEavesdropping is when an attacker passively intercepts communication between two parties. The attacker doesn't alter data but simply listens in, gathering sensitive information. Weak encryption ciphers and expired certificates can make it easier for bad actors to eavesdrop.\nData breaches:\nBreaches occur when a cybercriminal gains unauthorized access to (and steals sensitive data from) your system. SSL misconfigurations, like insecure redirects or the presence of mixed content, can both lead to data breaches.\nDesensitization:\nrepeating issues with expired or invalid SSL-certificates on your companies websites can desensitize your users against common cybersecurity practices. Months of cybersecurity awareness trainings drilled into them that websites without working SSL certificates pose a danger and should not be visited. Asking them to overlook the issue on your own websites can make them more receptive to phishing or fraud attempts later down the line since they are \"used to\" HTTPS-errors on your sites.\nChallenges in identifying SSL misconfigurations\nIdentifying SSL misconfigurations without a comprehensive\nExternal Attack Surface Management (EASM)\nsolution is challenging. The fact is most traditional security tools simply don't have the capacity to continuously monitor and analyze all of your organization's internet-facing assets. Combine this with the dynamic, ever-changing nature of digital environments — where assets are frequently added and updated — and it becomes even more difficult to effectively maintain secure SSL configurations. Specifically, for two reasons:\nTraditional security tools have limited capacity:\nMost conventional security tools are designed to monitor and protect internal networks and assets. However, they often lack the specialized capabilities to scan and analyze the wide array of internet-facing assets, including websites, web applications, APIs, and more, for SSL misconfigurations. Traditional tools can easily miss things like SSL certificate expirations and weak cipher suites, leaving your organization vulnerable.\nThe digital environment is always changing:\nYour organization's digital environment is dynamic as your team continually adds, removes, or updates content, applications, and services. And this constant change means you can inadvertently and easily introduce SSL misconfigurations.\nMitigating SSL misconfigurations with EASM\nTo take a proactive approach to managing and securing your organization's external attack surface (including SSL configurations), consider investing in an automated, cloud-based EASM solution that monitors all your known and unknown assets. The best solutions can:\nPerform continuous discovery and monitoring:\nInvest in a solution that scans and monitors all internet-facing assets for SSL misconfigurations, ensuring that any vulnerabilities are quickly identified and addressed.\nMonitor encryption certificates:\nYour chosen solution should also monitor SSL certificates for expiration dates, the certificate chain, TLS protocols, and issuers, preventing the use of insecure or expired certificates.\nBenefit from automated analysis:\nConsider a solution that automatically analyzes your SSL configuration and then identifies potential issues, ranking them based on their potential severity. This ongoing analysis and prioritization can help you better target your remediation efforts.\nReceive proactive alerts:\nYou don't know what you don't know. Find a solution that provides proactive alerts about SSL misconfigurations, allowing you to take swift action to mitigate potential security risks.\nTake a hands-off approach:\nFor the most convenient approach to securing your organization's external attack surface, consider a provider that offers managed EASM service. With a managed EASM provider, the vendor should provide continual 24/7 monitoring and connect with you regularly to review threats and remediate identified vulnerabilities.\nOne solution that checks all of these boxes is\nOutpost24's EASM platform\n. A cloud-based platform, that allows you to enhance your cyber resilience. The solution continually maps your organization's growing attack surface, automatically gathering and analyzing data for both your known and unknown assets as well as adding cyber threat intelligence feeds for a more comprehensive approach to cyber risk. Then, the platform offers a variety of potential remediation actions you can take to eliminate security gaps and secure your digital presence against SSL vulnerabilities.\nYour organization's internet-facing assets are ever-growing — and your attack surface is, too. Understand your attack surface and boost cyber resilience with Outpost24's Sweepatic EASM.\nContact us to learn more about how EASM can help mitigate Cyber Risk in your attack surface.\nFound this article interesting?\nThis article is a contributed piece from one of our valued partners.\nFollow us on\nTwitter\n\nand\nLinkedIn\nto read more exclusive content we post."
    },
    {
        "Title": "FIN7 Deploys Anubis Backdoor to Hijack Windows Systems via Compromised SharePoint Sites",
        "Link": "https://thehackernews.com/2025/04/fin7-deploys-anubis-backdoor-to-hijack.html",
        "Summary": "The financially motivated threat actor known as FIN7 has been linked to a Python-based backdoor called Anubis (not to be confused with an Android banking trojan of the same name) that can grant them remote access to compromised Windows systems.\n\"This malware allows attackers to execute remote shell commands and other system operations, giving them full control over an infected machine,\" Swiss",
        "Date": "2025-04-02",
        "Content": "The financially motivated threat actor known as FIN7 has been linked to a Python-based backdoor called Anubis (not to be confused with an\nAndroid banking trojan\nof the same name) that can grant them remote access to compromised Windows systems.\n\"This malware allows attackers to execute remote shell commands and other system operations, giving them full control over an infected machine,\" Swiss cybersecurity company PRODAFT\nsaid\nin a technical report of the malware.\nFIN7, also called Carbon Spider, ELBRUS, Gold Niagara, Sangria Tempest, and Savage Ladybug, is a\nRussian cybercrime group\nknown for its\never-evolving\nand\nexpanding\nset of malware families for obtaining initial access and data exfiltration. In recent years, the threat actor is said to have transitioned to a ransomware affiliate.\nIn July 2024, the group was observed using various online aliases to advertise a tool called AuKill (aka AvNeutralizer) that's capable of terminating security tools in a likely attempt to diversify its monetization strategy.\nAnubis is believed to be propagated via malspam campaigns that typically entice victims into executing the payload hosted on compromised SharePoint sites.\nDelivered in the form of a ZIP archive, the entry point of the infection is a Python script that's designed to decrypt and execute the main obfuscated payload directly in memory. Once launched, the backdoor establishes communications with a remote server over a TCP socket in Base64-encoded format.\nThe responses from the server, also Base64-encoded, allow it to gather the IP address of the host, upload/download files, change the current working directory, grab environment variables, alter Windows Registry, load DLL files into memory using PythonMemoryModule, and terminate itself.\nIn an independent analysis of Anubis, German security company GDATA\nsaid\nthe backdoor also supports the ability to run operator-provided responses as a shell command on the victim system.\n\"This enables attackers to perform actions such as keylogging, taking screenshots, or stealing passwords without directly storing these capabilities on the infected system,\" PRODAFT said. \"By keeping the backdoor as lightweight as possible, they reduce the risk of detection while maintaining flexibility for executing further malicious activities.\"\nFound this article interesting?  Follow us on\nTwitter\n\nand\nLinkedIn\nto read more exclusive content we post."
    },
    {
        "Title": "New Malware Loaders Use Call Stack Spoofing, GitHub C2, and .NET Reactor for Stealth",
        "Link": "https://thehackernews.com/2025/04/new-malware-loaders-use-call-stack.html",
        "Summary": "Cybersecurity researchers have discovered an updated version of a malware loader called Hijack Loader that implements new features to evade detection and establish persistence on compromised systems.\n\"Hijack Loader released a new module that implements call stack spoofing to hide the origin of function calls (e.g., API and system calls),\" Zscaler ThreatLabz researcher Muhammed Irfan V A said in",
        "Date": "2025-04-02",
        "Content": "Cybersecurity researchers have discovered an updated version of a malware loader called Hijack Loader that implements new features to evade detection and establish persistence on compromised systems.\n\"Hijack Loader released a new module that implements call stack spoofing to hide the origin of function calls (e.g., API and system calls),\" Zscaler ThreatLabz researcher Muhammed Irfan V A\nsaid\nin an analysis. \"Hijack Loader added a new module to perform anti-VM checks to detect malware analysis environments and sandboxes.\"\nHijack Loader, first discovered in 2023, offers the ability to deliver second-stage payloads such as information stealer malware. It also comes with a variety of modules to bypass security software and inject malicious code. Hijack Loader is tracked by the broader cybersecurity community under the names DOILoader, GHOSTPULSE, IDAT Loader, and SHADOWLADDER.\nIn October 2024, HarfangLab and Elastic Security Labs\ndetailed\nHijack Loader campaigns that leveraged legitimate code-signing certificates as well as the infamous ClickFix strategy for distributing the malware.\nThe latest iteration of the loader comes with a number of improvements over its predecessor, the most notable being the addition of call stack spoofing as an evasion tactic to conceal the origin of API and system calls, a method recently also embraced by another malware loader known as\nCoffeeLoader\n.\n\"This technique uses a chain of EBP pointers to traverse the stack and conceal the presence of a malicious call in the stack by replacing actual stack frames with fabricated ones,\" Zscaler said.\nAs with previous versions, the Hijack Loader leverages the\nHeaven's Gate technique\nto execute 64-bit direct syscalls for process injection. Other changes include a revision to the list of blocklisted processes to include \"avastsvc.exe,\" a component of Avast Antivirus, to delay execution by five seconds.\nThe malware also incorporates two new modules, namely ANTIVM for detecting virtual machines and modTask for setting up persistence via scheduled tasks.\nThe findings show that Hijack Loader continues to be actively maintained by its operators with an intent to complicate analysis and detection.\nSHELBY Malware Uses GitHub for Command-and-Control\nThe development comes as Elastic Security Labs detailed a new malware family dubbed SHELBY that uses GitHub for command-and-control (C2), data exfiltration, and remote control. The activity is being tracked as REF8685.\nThe attack chain involves the use of a phishing email as a starting point to distribute a ZIP archive containing a .NET binary that's used to execute a DLL loader tracked as SHELBYLOADER (\"HTTPService.dll\") via DLL side-loading. The email messages were delivered to an Iraq-based telecommunications firm through a highly targeted phishing email sent from within the targeted organization.\nThe loader subsequently initiates communications with GitHub for C2 to extract a specific 48-byte value from a file named \"License.txt\" in the attackers-controlled repository. The value is then used to generate an AES decryption key and decipher the main backdoor payload (\"HTTPApi.dll\") and load it into memory without leaving detectable artifacts on disk.\n\"SHELBYLOADER utilizes sandbox detection techniques to identify virtualized or monitored environments,\" Elastic\nsaid\n. \"Once executed, it sends the results back to C2. These results are packaged as log files, detailing whether each detection method successfully identified a sandbox environment.\"\nThe SHELBYC2 backdoor, for its part, parses commands listed in another file named \"Command.txt\" to download/upload files from/to a GitHub repository, load a .NET binary reflectively, and run PowerShell commands. What's notable here is the C2 communication occurs through commits to the private repository by making use of a Personal Access Token (PAT).\n\"The way the malware is set up means that anyone with the PAT (Personal Access Token) can theoretically fetch commands sent by the attacker and access command outputs from any victim machine,\" the company said. \"This is because the PAT token is embedded in the binary and can be used by anyone who obtains it.\"\nEmmenhtal Spreads SmokeLoader via 7-Zip Files\nPhishing emails bearing payment-themed lures have also been observed delivering a malware loader family codenamed\nEmmenhtal\nLoader (aka PEAKLIGHT), which acts as a conduit to deploy another malware known as\nSmokeLoader\n.\n\"One notable technique observed in this SmokeLoader sample is the use of .NET Reactor, a commercial .NET protection tool used for obfuscation and packing,\" GDATA\nsaid\n.\n\"While SmokeLoader has historically leveraged packers like Themida, Enigma Protector, and custom crypters, the use of .NET Reactor aligns with trends seen in other malware families, particularly stealers and loaders, due to its strong anti-analysis mechanisms.\"\nFound this article interesting?  Follow us on\nTwitter\n\nand\nLinkedIn\nto read more exclusive content we post."
    },
    {
        "Title": "Genetic data site openSNP to close and delete data over privacy concerns",
        "Link": "https://www.bleepingcomputer.com/news/security/genetic-data-site-opensnp-to-close-and-delete-data-over-privacy-concerns/",
        "Summary": "The openSNP project, a platform for sharing genetic and phenotypic data, will shut down on April 30, 2025, and delete all user submissions over privacy concerns and the risk of misuse by authoritarian governments. [...]",
        "Date": "2025-04-02",
        "Content": "The openSNP project, a platform for sharing genetic and phenotypic data, will shut down on April 30, 2025, and delete all user submissions over privacy concerns and the risk of misuse by authoritarian governments.\nThe decision was announced earlier this week by co-founder Bastian Greshake Tzovaras, who expressed concerns about how personal genomics data is subject to abuse today and how fundamentally the landscape has changed over the last 14 years.\nOpenSNP is a free and\nopen-source\nplatform where individuals can upload and share their genetic and phenotype data for research and educational purposes.\nIts original goal was to democratize access to genetic data, breaking the monopoly of commercial DNA testing companies and enabling researchers or regular people to explore human genetic data without financial or institutional barriers.\nOver the years, openSNP became one of the largest repositories of its kind, used in research, education, and even community-led investigations like\ndebunking\nflawed CFS genetic studies.\nAlthough not affiliated with 23andMe, the vast majority of contributions it received were from users who had their genomes sequenced by 23andMe.\nWith\n23andMe filing for bankruptcy\n, the flow of new data submitted to openSNP has essentially stopped, and Tzovaras doesn't expect this to change any time soon.\nThe co-founder also voiced concern that retaining openSNP's data could invite misuse, especially as private forensic firms, law enforcement agencies, and governments have become increasingly aggressive in seeking access to such information on the grounds of pseudo-scientific arguments.\n\"The risk/benefit calculus of providing free & open access to individual genetic data in 2025 is very different compared to 14 years ago,\"\nexplained Tzovaras\n.\n\"And so, sunsetting openSNP – along with deleting the data stored within it – feels like it is the most responsible act of stewardship for these data today.\"\nOpenSNP constituted a rare example of how open-source projects can successfully operate on a low budget, having repeatedly rejected offers from corporations to sell control of the data. However, the organization believes it is now too risky to continue, given the changes in the ethical, political, and societal climate.\nHence, the project will shut down at the end of the month, and all user submissions will be wiped.\nThe announcement does not explicitly instruct users to take any action such as deleting their data, so no manual actions are required.\nHowever, those who want to keep a copy of their or other data for personal use have until April 30, 2025, to download it.\nThat said, anyone who has already downloaded the data has a copy forever, but removing the public, centralized source lowers discoverability and future access by scraping operations.\nTop 10 MITRE ATT&CK\n©\nTechniques Behind 93% of Attacks\nBased on an analysis of 14M malicious actions, discover the top 10 MITRE ATT&CK techniques behind 93% of attacks and how to defend against them.\nRead the Red Report 2025\nRelated Articles:\n23andMe files for bankruptcy, customers advised to delete DNA data\nRecent GitHub supply chain attack traced to leaked SpotBugs token\nMicrosoft uses AI to find flaws in GRUB2, U-Boot, Barebox bootloaders\nNew npm attack poisons local packages with backdoors\nNew Akira ransomware decryptor cracks encryptions keys using GPUs"
    },
    {
        "Title": "Verizon Call Filter API flaw exposed customers' incoming call history",
        "Link": "https://www.bleepingcomputer.com/news/security/verizon-call-filter-api-flaw-exposed-customers-incoming-call-history/",
        "Summary": "A vulnerability in Verizon's Call Filter feature allowed customers to access the incoming call logs for another Verizon Wireless number through an unsecured API request. [...]",
        "Date": "2025-04-02",
        "Content": "A vulnerability in Verizon's Call Filter feature allowed customers to access the incoming call logs for another Verizon Wireless number through an unsecured API request.\nThe flaw was discovered by security researcher\nEvan Connelly\non February 22, 2025, and was fixed by Verizon sometime in the following month. However, the total period of exposure is unknown.\nVerizon's Call Filter app is a free utility that offers users spam detection and automatic call blocking. A paid version (Plus) adds a spam lookup and risk meter, the ability to apply blocks by type of caller, and receive caller ID on unknown numbers.\nThe free version of the app comes\npre-installed and enabled by default\non eligible Android and iOS devices bought directly from Verizon, and is believed to be used on millions of devices.\nConnelly told BleepingComputer that he only tested\nthe iOS app\n. However, he noted that\nthe Android app\nwas also very likely impacted by the same bug, as the issue was with the feature's API rather than the apps themselves.\nExposing call histories\nWhen using the Call Filter app, Connelly discovered that the app would connect to an API endpoint, https://clr-aqx.cequintvzwecid.com/clr/callLogRetrieval, to retrieve the logged-in user's incoming call history and display it in the app.\n\"This endpoint requires a JWT (JSON Web Token) in the Authorization header using the Bearer scheme and uses an X-Ceq-MDN header to specify a cell phone number to retrieve call history logs for,\" explains Connelly.\n\"A JWT has three parts: header, payload, and signature. It's often used for authentication and authorization in web apps.\"\nConnelly says the payload includes various data, including the phone number of the logged-in user making the request to the API.\nJWT payload\nSource: Connelly\nHowever, the researcher discovered that the phone number in the JWT payload for the logged-in user was not verified against the phone number whose incoming call logs were being requested.\nAs a result, any user could send requests using their own valid JWT token, but replace the X-Ceq-MDN header value with another Verizon phone to retrieve their incoming call history.\nExample request sent to the vulnerable API\nSource: evanconnelly.github.io\nThis flaw is particularly sensitive for high-value targets like politicians, journalists, and law enforcement agents, as their sources, contacts, and daily routines could be mapped out.\n\"Call metadata might seem harmless, but in the wrong hands, it becomes a powerful surveillance tool. With unrestricted access to another user's call history, an attacker could reconstruct daily routines, identify frequent contacts, and infer personal relationships,\" explained Connelly.\nIt is unclear if rate limiting was in place to prevent mass scraping for millions of subscribers, but Connolly told BleepingComputer he saw no indication of such a mechanism or an API gateway that usually implements a security feature like this.\nPoor security practices\nAlthough the researcher commends Verizon for its prompt response to his disclosure, he highlighted worrying practices the telecom firm has followed in handling subscribers' call data.\nThe vulnerable API endpoint used by Call Filter appears to be hosted on a server owned by a separate telecommunications technology firm called Cequint, which specializes in caller identification services.\nCequint's own website is offline, and public information about them is limited, raising concerns about how sensitive Verizon call data is handled.\nBleepingComputer contacted Verizon to ask when the flaw was introduced, if it was seen exploited in the past, and if it impacted all Call Filter users but has not received a response at this time.\nUpdate 4/3\n- A Verizon spokersperson has sent BleepingComputer the below statement:\n\"Verizon was made aware of this vulnerability and worked with the third-party app owner on a fix and patch that was pushed in mid-March. While there was no indication that the flaw was exploited, the issue was resolved and only impacted iOS devices. Verizon appreciates the responsible disclosure of the finding by the researcher and takes the security very seriously.\"\nTop 10 MITRE ATT&CK\n©\nTechniques Behind 93% of Attacks\nBased on an analysis of 14M malicious actions, discover the top 10 MITRE ATT&CK techniques behind 93% of attacks and how to defend against them.\nRead the Red Report 2025\nRelated Articles:\nCrypto-stealing apps found in Apple App Store for the first time\nApple backports zero-day patches to older iPhones and Macs\nNew Android malware uses Microsoft’s .NET MAUI to evade detection\nMalicious Android 'Vapor' apps on Google Play installed 60 million times\nNew North Korean Android spyware slips onto Google Play"
    },
    {
        "Title": "GitHub expands security tools after 39 million secrets leaked in 2024",
        "Link": "https://www.bleepingcomputer.com/news/security/github-expands-security-tools-after-39-million-secrets-leaked-in-2024/",
        "Summary": "Over 39 million secrets like API keys and account credentials were leaked on GitHub throughout 2024, exposing organizations and users to significant security risks. [...]",
        "Date": "2025-04-02",
        "Content": "GitHub announced updates to its Advanced Security platform after it detected over 39 million leaked secrets in repositories during 2024, including API keys and credentials, exposing users and organizations to serious security risks.\nIn a new report by GitHub, the development company says the 39 million secrets were found through its\nsecret scanning service\n, a security feature that detects API keys, passwords, tokens, and other secrets in repositories.\n\"Secret leaks remain one of the most common—and preventable—causes of security incidents,\"\nreads GitHub's announcement\n.\n\"As we develop code faster than ever previously imaginable, we're leaking secrets faster than ever, too.\"\nThis is happening despite GitHub's targeted protection measures like \"Push Protection,\" which was introduced in April 2022 and was\nactivated by default\non all public repositories in February 2024.\nAccording to GitHub, the main reasons why secrets continue to leak are the prioritization of convenience by developers who handle secrets during commits and accidental repository exposure through git history.\nGitHub revamps Advanced Security\nGitHub announced several new measures and enhancements to existing systems to mitigate secret leaks on the platform.\n\"As of today, our security products are available to purchase as standalone products for enterprises, enabling development teams to scale security quickly,\" explained GitHub.\n\"Previously, investing in secret scanning and push protection required purchasing a larger suite of security tools, which made it too expensive for many organizations.\n\"This change ensures scalable security with Secret Protection and Code Security is no longer out of reach for many organizations.\"\nThe GitHub Advanced Security changes are summarized as follows:\nStandalone Secret Protection and Code Security\n– Now available as separate products, these tools no longer require a full GitHub Advanced Security license, making them more affordable for smaller teams.\nFree organization-wide secret risk assessment\n– A point-in-time scan that checks all repositories (public, private, internal, and archived) for exposed secrets, free for all GitHub organizations.\nPush protection with delegated bypass controls\n– Enhanced push protection scans for secrets before code is pushed and allows organizations to define who can bypass the protection, adding policy-level control.\nCopilot-powered secret detection\n– GitHub now uses AI via Copilot to detect unstructured secrets like passwords, improving accuracy and lowering false positives.\nImproved detection via cloud provider partnerships\n– GitHub works with providers like AWS, Google Cloud, and OpenAI to build more accurate secret detectors and respond faster to leaks.\nApart from GitHub's initiatives and improvements, users are also given a list of recommended actions to protect themselves from secret leaks.\nFirst, it is suggested that Push Protection be enabled at the repository, organization, or enterprise level to block secrets before they're pushed to a repository.\nGitHub also highlights the importance of reducing the risk by eliminating hardcoded secrets from source code altogether, instead using environment variables, secret managers, or vaults to store them.\nThe platform suggests using tools that integrate with CI/CD pipelines and cloud platforms to handle secrets programmatically, reducing human interaction that can introduce errors and exposure.\nFinally, GitHub users are recommended to review the '\nBest Practices\n' guide and ensure they appropriately manage secrets end-to-end.\nTop 10 MITRE ATT&CK\n©\nTechniques Behind 93% of Attacks\nBased on an analysis of 14M malicious actions, discover the top 10 MITRE ATT&CK techniques behind 93% of attacks and how to defend against them.\nRead the Red Report 2025\nRelated Articles:\nGitHub Action supply chain attack exposed secrets in 218 repos\nGitHub Action hack likely led to another in cascading supply chain attack\nSupply chain attack on popular GitHub Action exposes CI/CD secrets\nRoyal Mail investigates data leak claims, no impact on operations\nMicrosoft uses AI to find flaws in GRUB2, U-Boot, Barebox bootloaders"
    },
    {
        "Title": "Microsoft adds hotpatching support to Windows 11 Enterprise",
        "Link": "https://www.bleepingcomputer.com/news/microsoft/microsoft-adds-hotpatching-support-to-windows-11-enterprise/",
        "Summary": "Microsoft has announced that hotpatch updates are now available for business customers using Windows 11 Enterprise 24H2 on x64 (AMD/Intel) systems, starting today. [...]",
        "Date": "2025-04-02",
        "Content": "Microsoft has announced that hotpatch updates are now available for business customers using Windows 11 Enterprise 24H2 on x64 (AMD/Intel) systems, starting today.\nOn devices where hotpatching is available, Windows allows users to install OS security updates by downloading and installing them in the background without rebooting the device.\nTo do that, it deploys the security updates by patching the in-memory code of running processes without restarting them after each installation.\n\"With hotpatch updates, you can quickly take measures to help protect your organization from cyberattacks, while minimizing user disruptions. You'll first create a hotpatch-enabled quality update policy in Windows Autopatch through the Microsoft Intune console,\"\nMicrosoft said\nin a Wednesday message center update.\n\"Devices managed by this policy will be offered hotpatch updates in a quarterly cycle. Eight months out of twelve, you won't need to restart the device for the security update to take effect.\"\nEligible Windows 11 Enterprise 24H2 devices managed by this policy will be offered hotpatch updates quarterly, following the same ring deployment schedule as standard updates.\nWindows hotpatch timeline (Microsoft)\nTo enable hotpatching for Windows client devices, you will need a Microsoft subscription (i.e., Windows 11 Enterprise E3, E5, or F3, Windows 11 Education A3 or A5, or a Windows 365 Enterprise subscription) and a Windows 11 Enterprise 24H2 PC with the current baseline update installed.\nOther requirements include an x64 AMD64 or Intel CPU,\nVirtualization-based Security (VBS)\nenabled, and Microsoft Intune to manage hotpatch update deployment with a hotpatch-enabled Windows quality update policy.\nMicrosoft says that hotpatch updates are still in public preview for Arm64 devices. However, admins can still turn off CHPE support by setting a HotPatchRestrictions registry key to ensure that these devices are eligible until the feature becomes available:\n▪ Path: HKLM\\SYSTEM\\CurrentControlSet\\Control\\Session Manager\\Memory Management\r\n▪ DWORD Key value: HotPatchRestrictions=1\nIf all prerequisites are met to receive hotpatch updates, you can enable or disable them by going to Devices > 'Windows updates'> 'Create Windows quality update policy' in the Microsoft Intune admin center to create a Windows quality update policy as shown in the screenshot embedded below.\nEnabling hotpatching via Intune admin center (Microsoft)\n​\"The Windows quality update policy can auto-detect if your targeted devices are eligible for hotpatch updates,\" Microsoft added today.\n\"Devices running Windows 10 and Windows 11, version 23H2 and lower will continue to receive the standard monthly security updates, helping ensure that your ecosystem stays protected and productive.\"\nMicrosoft first added\nWindows Hotpatch\nsupport to Windows Server Azure Edition core virtual machines, making it generally available in February 2022 for systems running\nWindows Server 2022 Datacenter: Azure Edition\n.\nThe company also started testing it in public preview for Windows Server 2025\nin September 2024\nand on Windows 11 24H2 and Windows 365 two months later,\nin November 2024\n.\nTop 10 MITRE ATT&CK\n©\nTechniques Behind 93% of Attacks\nBased on an analysis of 14M malicious actions, discover the top 10 MITRE ATT&CK techniques behind 93% of attacks and how to defend against them.\nRead the Red Report 2025\nRelated Articles:\nWindows 11 KB5052093 update released with 33 changes and fixes\nMicrosoft shares workaround for Windows security update issues\nWindows 11 update breaks Veeam recovery, causes connection errors\nWindows 11 KB5053598 & KB5053602 cumulative updates released\nMicrosoft March 2025 Patch Tuesday fixes 7 zero-days, 57 flaws"
    },
    {
        "Title": "Royal Mail investigates data leak claims, no impact on operations",
        "Link": "https://www.bleepingcomputer.com/news/security/royal-mail-investigates-data-leak-claims-no-impact-on-operations/",
        "Summary": "​Royal Mail is investigating claims of a security breach after a threat actor leaked over 144GB of data allegedly stolen from the company's systems. [...]",
        "Date": "2025-04-02",
        "Content": "​Royal Mail is investigating claims of a security breach after a threat actor leaked over 144GB of data allegedly stolen from the company's systems.\nWhen asked to confirm the authenticity of the leaked data, a Royal Mail spokesperson told BleepingComputer that the British postal service is aware of an incident at Spectos GmbH, a third-party data collection and analytics service provider.\n\"We are aware of an incident which is alleged to have affected Spectos, a supplier of Royal Mail. We are working with the company to investigate the issue and establish what impact there may be regarding their data,\" BleepingComputer was told. \"We can confirm there has been no impact on Royal Mail operations and services continue to function as normal.\"\nSpectos also confirmed in a statement shared with BleepingComputer that its systems were breached on March 29, and the attackers gained access to customer data.\n\"Spectos GmbH has been the target of an ongoing cyber attack since March 29, 2025. According to the current status, unauthorized access to systems and personal customer data has occurred. The exact scope of the incident is currently the subject of intensive forensic investigations,\" a spokesperson told BleepingComputer.\nThe threat actor behind this leak (who uses the \"GHNA\" handle on BreachForums) released 16,549 files allegedly containing Royal Mail customers' personally identifiable information (including names, addresses, planned delivery dates, and more) and other confidential documents.\nGHNA says the leaked documents also include Mailchimp mailing lists, datasets containing delivery/post office locations, the WordPress SQL database for mail agents.uk, internal Zoom meeting video recordings between Spectos and the Royal Mail Group, and more.\nRoyal Mail leak on BreachForums (BleepingComputer)\n​Breached using stolen credentials\nWhile Royal Mail and Spectos have yet to share more information on the breach, cybersecurity company Hudson Rock says the attackers gained access to Royal Mail systems using the credentials of a Spectos employee compromised in a 2021 info stealer malware incident.\n\"In this case, the infected Spectos employee's credentials provided a gateway to Royal Mail Group's systems,\"\nHudson Rock CTO Alon Gal said\n. \"The stolen data sat dormant until recently, when it was weaponized in these high-profile leaks.\"\nStolen Spectos credentials (Hudson Rock)\nThis isn't the first time the British postal service has dealt with a security breach since it was founded over 500 years ago. The British postal service\nwas also breached two years ago\nin a cyberattack claimed by the notorious LockBit ransomware operation.\nThe January 2023 breach forced the company to\nhalt international shipping services\ndue to what it described as a \"cyber incident\" causing \"severe service disruption.\" Royal Mail restored these services\nthree weeks after\nthe ransomware attack impacted its operations.\nAnother outage hit Royal Mail\nin November 2022\n, which took down tracking services for more than 24 hours.\nTop 10 MITRE ATT&CK\n©\nTechniques Behind 93% of Attacks\nBased on an analysis of 14M malicious actions, discover the top 10 MITRE ATT&CK techniques behind 93% of attacks and how to defend against them.\nRead the Red Report 2025\nRelated Articles:\nOracle denies breach after hacker claims theft of 6 million data records\nSuspected Desorden hacker arrested for breaching 90 organizations\nOrange Group confirms breach after hacker leaks company documents\nHacker leaks account data of 12 million Zacks Investment users\nGitHub expands security tools after 39 million secrets leaked in 2024"
    },
    {
        "Title": "ChatGPT is down worldwide with something went wrong error",
        "Link": "https://www.bleepingcomputer.com/news/artificial-intelligence/chatgpt-is-down-worldwide-with-something-went-wrong-error/",
        "Summary": "ChatGPT, the famous artificial intelligence chatbot that allows users to converse with various personalities and topics, has connectivity issues worldwide. [...]",
        "Date": "2025-04-02",
        "Content": "ChatGPT isn't working for millions of users worldwide. While it's able to respond to the first message in a conversation, it doesn't work when you follow up or respond to ChatGPT.\nYou'll run into \"Something went wrong while generating the response. If this issue persists please contact us through our help center at help.openai.com.\" error message when you try to chat with the AI.\nChatGPT is showing errors\nSource: BleepingComputer\nIf you click the Retry button, ChatGPT will recommend you refresh the page, but the error will not go away even after reloading the page.\nAccording to\nDownDetector\n, this outage started within the last 30 minutes.\nChatGPT is currently experiencing an outage in the U.S, Europe, India, Japan, Australia, and other parts of the world.\nIn an update to its support document, OpenAI confirmed it's aware of the reports and is working on a mitigation.\n“We have identified that users are experiencing elevated errors for the impacted services. We are working on implementing a mitigation,\" OpenAI noted.\nUpdate: OpenAI has resolved issues with ChatGPT.\nTop 10 MITRE ATT&CK\n©\nTechniques Behind 93% of Attacks\nBased on an analysis of 14M malicious actions, discover the top 10 MITRE ATT&CK techniques behind 93% of attacks and how to defend against them.\nRead the Red Report 2025\nRelated Articles:\nOpenAI says Deep Research is coming to ChatGPT free \"very soon\"\nClaude is testing ChatGPT-like Deep Research feature Compass\nOpenAI's GPT 4.5 spotted in Android beta, launch imminent\nOpenAI bans ChatGPT accounts used by North Korean hackers\nMicrosoft uses AI to find flaws in GRUB2, U-Boot, Barebox bootloaders"
    },
    {
        "Title": "Police shuts down KidFlix child sexual exploitation platform",
        "Link": "https://www.bleepingcomputer.com/news/security/police-shuts-down-kidflix-child-sexual-exploitation-platform/",
        "Summary": "Kidflix, one of the largest platforms used to host, share, and stream child sexual abuse material (CSAM) on the dark web, was shut down on March 11 following a joint action coordinated by German law enforcement. [...]",
        "Date": "2025-04-02",
        "Content": "Kidflix, one of the largest platforms used to host, share, and stream child sexual abuse material (CSAM) on the dark web, was shut down on March 11 following a joint action coordinated by German law enforcement.\nDubbed Operation Stream, this joint international investigation is led by the State Criminal Police of Bavaria (Bayerisches Landeskriminalamt) and the Bavarian Central Office for the Prosecution of Cybercrime (ZCB).\nThe joint operation was also supported by Europol analysts from the European Cybercrime Centre (EC3), who analyzed thousands of videos, providing evidence to facilitate the investigation.\nOperation Stream started in 2022 and has so far led to 79 arrests, 1,393 suspects identified, and over 3,000 electronic devices seized between March 10 and March 23, 2025.\n\"A total of 1.8 million users worldwide logged on to the platform between April 2022 and March 2025. On 11 March 2025, the server, which contained around 72,000 videos at the time, was seized by German and Dutch authorities. Some of those arrested not only uploaded and watched videos but also abused children,\"\nEuropol said\n.\nAccording\nto the Dutch National Police\n, information about the suspects has been shared with investigation authorities in 35 countries, who had official warning conversations with them between March 10 and March 21.\nMany of the suspects identified during Operation Stream were cross-referenced with Europolțs databases, showing that a large portion of those involved in child sexual exploitation are often repeat offenders already on law enforcement's radar.\nKidFlix seizure banner (Europol)\n​Launched in 2021, Kidflix hosted over 91,000 unique videos with a total running time of 6,288 hours while it was active. Roughly 3.5 new videos were uploaded to the dark web platform every hour, many previously unknown to law enforcement.\n\"Unlike other known platforms of this kind, Kidflix not only enabled users to download CSAM but also to stream video files. Users made payments using cryptocurrencies, which were subsequently converted into tokens,\" Europol added.\n\"By uploading CSAM, verifying video titles and descriptions and assigning categories to videos, offenders could earn tokens, which were then used to view content. Each video was uploaded in multiple versions – low, medium and high quality – allowing criminals to preview the content and pay a fee to unlock higher quality versions.\"\nIn February, law enforcement agencies from 19 countries\nalso arrested 25 suspects\nlinked to a criminal ring distributing AI-generated child sexual abuse material. During Operation Cumberland, investigators identified 273 suspected members of this criminal network and seized 173 electronic devices.\nEuropol launched the\nStop Child Abuse – Trace An Object initiative\nin 2017, allowing people to provide information that can help cases of child sexual abuse by recognizing objects linked to ongoing investigations. The Australian Federal Police also\nlaunched its own version\nof the Stop Child Abuse initiative in March 2021, focusing on the Asia Pacific region.\nTop 10 MITRE ATT&CK\n©\nTechniques Behind 93% of Attacks\nBased on an analysis of 14M malicious actions, discover the top 10 MITRE ATT&CK techniques behind 93% of attacks and how to defend against them.\nRead the Red Report 2025\nRelated Articles:\nPolice arrests suspects tied to AI-generated CSAM distribution ring\nGarantex crypto exchange admin arrested while on vacation\nCybercrime 'crew' stole $635,000 in Taylor Swift concert tickets\nSuspected Desorden hacker arrested for breaching 90 organizations\nPolice arrests 2 Phobos ransomware suspects, seizes 8Base sites"
    },
    {
        "Title": "The Reality Behind Security Control Failures—And How to Prevent Them",
        "Link": "https://www.bleepingcomputer.com/news/security/the-reality-behind-security-control-failures-and-how-to-prevent-them/",
        "Summary": "Most orgs only discover their security controls failed after a breach. With OnDefend's continuous validation, you can test, measure, and prove your defenses work—before attackers exploit blind spots. [...]",
        "Date": "2025-04-02",
        "Content": "There’s a clear gap between expectation and reality when it comes to security controls.\nDespite deploying best-in-class security tools and building capable teams, many organizations discover the truth only after a breach: their controls weren’t working as expected.\nThink of changing a lightbulb—you turn it on to check if it works. Security controls rarely get the same validation. Instead, success criteria become “don’t break production,” which doesn’t actually test whether the security controls are effective.\nIt’s not for lack of trying, but traditional methods—such as compliance audits and penetration tests—don’t fully answer the question, “\nWould we win?\n” if attacked.\nAs a result, blind spots persist.\nTraditional Security Testing Falls Short\nCompliance audits\nfocus on policy and process but rarely engage in operational assurance testing that confirms, “\nDoes this actually work as expected?\n”\nAnswering “Do you have antivirus software?” is very different from “How long does it take for a malicious file to be removed and your team to be alerted?”\nPenetration tests can highlight security gaps but often reflect a specific attack path chosen by the testers rather than a comprehensive evaluation of all potential failure points.\nThe end result? Gaps (or blind spots) that typically aren’t discovered until someone else finds them for you.\nFive Most Common Reasons Security Controls Fail\nFailures occur in both security tools—such as Secure Email Gateways (SEGs), Endpoint Detection and Response (EDR), and Security Information and Event Management (SIEM) systems—and in security teams, whether in-house or managed detection and response (MDR) providers.\nWe can categorize these into organizations threat prevention, detection, and response investments.\nA Case Study: How a Leading Healthcare System Proactively Removed Security Control Failures\nLearn how OnDefend’s BlindSPOT breach and attack simulation (BAS) managed service helped a major U.S. healthcare provider validate security controls, hold vendors accountable, reduce risk and protect patient data.\nDownload the Case Study\nTop Five Causes of Threat Prevention, Detection, and Response Failures\nPolicy Sprawl –\nOften the ideal policy from a security perspective clashes with the needs for authorized activity. This means your population within the security tool gets spread across a variety of policies with different settings and rules, often with significant differences to the overall performance. We often see organizations develop very well-crafted policies for their EDR tools and then realize that the majority of the fleet population is in the default policy, missing the opportunity to capitalize on the work performed by the team.\nUnintended Config Changes\n– False positive alerts consume most of a SOC analysts time.  What if the configuration changes to reduce this noise accidentally silences true positive events as well?  Errors in the alert query or accidentally applying the exception to all systems happen frequently.\nAbility to Execute the Playbook –\nMost organizations have done an excellent job building their threat hunting and incident response playbooks.  But do the analysts ever get a chance to execute those activities in production?  If you have an expectation that a SOC analyst can execute a fleet-wide hunt for a file with a given hash value, chances are not everyone on the team can do that correctly without ongoing training.\nUndersized Deployment –\nThe security tool deployment fit initially, but as the environment grows, do they have the capacity and licensing to keep up? It’s not uncommon for undersized deployments to result in very long processing times, which translates to delays in your team being alerted to malicious activity, sometimes hours.\nReal-World Example\nA customer had added some new data sources to the collected telemetry, these new logs coming from end-user devices across the fleet.  The sudden increase in logging overwhelmed the SIEM (Security Information and Event Management) system they had, creating a massive backlog of logs to be processed.  The alerts they were looking for would arrive – 6 hours after the event had occurred. It was only when automated testing was introduced that this issue was discovered.\nChanges around the Tools –\nOften the security tool itself works great – but the environment around the tooling has changed to the point that the tool is no longer effective.\nReal-World Example\nAn organization hired a third party for security monitoring, including a network IDS analyzing traffic from core routers. After an attack went undetected, they asked the vendor why—only to learn a network change months earlier had cut off traffic to the IDS. Despite receiving no data for months, it never triggered an alert or error.\nThe Need for Continuous Validation\nThe only way to combat these failures is by regularly testing security detection processes. To scale this effectively, control tests must be automated pointing out defects while cataloging successes and calculating key metrics such as\nMean Time to Detect (MTTD)\nand\nMean Time to Respond (MTTR)\n.\nTransitioning Continuous Testing Mindset\nManaged Breach & Attack Simulation (BAS) services provide ongoing assurance without adding operational burden. When combined with penetration testing, BAS ensures a more comprehensive security strategy.\nHolding vendors accountable is another key step—continuous testing provides data-driven evidence to measure vendor performance against SLAs, helping organizations demand better service, renegotiate contracts, or compare solutions before making purchasing decisions. Lastly, security leaders must communicate their effectiveness in business terms, using metrics like detection rates, response times, and financial risk reduction to quantify security’s value and in some cases leverage that data to lower cyber insurance premiums.\nTrust but Verify Your Security Investments Will Work\nSecurity leaders have always sought assurance that their cybersecurity investments perform as intended. With corporate stakeholders demanding measurable proof, continuous security validation bridges security, risk reduction, and business objectives—turning cybersecurity from a cost center into a strategic enabler.\nOnDefend’s Ransomware Defense Validation\nmanaged service enables security teams to continuously test and validate their security controls, ensuring real-time visibility into potential blind spots.\nDon’t let the bad guys do the validating for you. Stop assuming your controls will work—prove it.\nSee the difference continuous validation can make within your security program.\nLearn More About a Customized Defense Validation Program\n.\nSponsored and written by\nOnDefend\n."
    },
    {
        "Title": "Counterfeit Android devices found preloaded with Triada malware",
        "Link": "https://www.bleepingcomputer.com/news/security/counterfeit-android-devices-found-preloaded-with-triada-malware/",
        "Summary": "A new version of the Triada trojan has been discovered preinstalled on thousands of new Android devices, allowing threat actors to steal data as soon as they are set up. [...]",
        "Date": "2025-04-02",
        "Content": "A new version of the Triada trojan has been discovered preinstalled on thousands of new Android devices, allowing threat actors to steal data as soon as they are set up.\nKaspersky researchers report that this campaign mainly impacts Russian users, with at least 2,600 confirmed infections from March 13 to 27, 2025, based on visibility from its mobile protection tools.\nThe security researchers noted that Triada was found on counterfeit versions of popular smartphone models sold at online stores at discounted prices to attract the interest of unsuspecting buyers.\nTriada is a modular Android malware first discovered in 2016, considered a pioneer at the time for operating almost entirely in the device's RAM to evade detection.\nSince then, there have been\nmultiple reports\nof Triada hiding in the firmware of\nlow-cost Android phones\nsold through dubious unofficial retail channels, making it a stealthy and also persistent threat that can't be removed without reflashing the ROM.\nKaspersky's latest report indicates that the newest version of Triada remains highly evasive, hiding in Android's system framework and copying itself to every process on the smartphone.\nThe latest Triada malware variant performs the following actions on infected devices:\nSteals accounts from messengers and social media\nSends and deletes messages via WhatsApp and Telegram to impersonate users\nHijacks cryptocurrency by replacing wallet addresses in apps\nTracks browsing activity and swaps links\nSpoofs phone numbers during calls to reroute conversations\nIntercepts, sends, and deletes SMS messages\nEnables premium SMS to charge paid services\nDownloads and runs additional apps remotely\nBlocks network connections to evade detection or disrupt defenses\nTransaction analysis shows that the new Triada trojan has stolen at least $270,000 worth of cryptocurrency. However, the total amount stolen by the operation is unknown as it also involves the hard-to-trace Monero cryptocurrency.\nKaspersky isn't sure how the devices are infected with Triada but hypothesizes it's the result of a supply chain attack.\n\"Its [Triada's] new version is embedded into smartphone firmware before the devices even reach users,\"\ncommented Kaspersky's Dmitry Kalinin\n.\n\"It is likely that the supply chain is compromised at some point, so even the stores may not realize they're selling phones with Triada.\"\nTo mitigate this risk, only buy smartphones from authorized distributors.\nWhen in doubt, reflash your device using a clean\nsystem image from Google\n, or a trustworthy third-party ROM like\nLineageOS\nor\nGrapheneOS\n.\nTop 10 MITRE ATT&CK\n©\nTechniques Behind 93% of Attacks\nBased on an analysis of 14M malicious actions, discover the top 10 MITRE ATT&CK techniques behind 93% of attacks and how to defend against them.\nRead the Red Report 2025\nRelated Articles:\nNew Crocodilus malware steals Android users’ crypto wallet keys\nNew Android malware uses Microsoft’s .NET MAUI to evade detection\nMalicious Android 'Vapor' apps on Google Play installed 60 million times\nBadBox malware disrupted on 500K infected Android devices\nVo1d malware botnet grows to 1.6 million Android TVs worldwide"
    },
    {
        "Title": "Cisco warns of CSLU backdoor admin account used in attacks",
        "Link": "https://www.bleepingcomputer.com/news/security/cisco-warns-of-cslu-backdoor-admin-account-used-in-attacks/",
        "Summary": "Cisco warns admins to patch a critical Cisco Smart Licensing Utility (CSLU) vulnerability, which exposes a built-in backdoor admin account now used in attacks. [...]",
        "Date": "2025-04-02",
        "Content": "Cisco has warned admins to patch a critical Cisco Smart Licensing Utility (CSLU) vulnerability, which exposes a built-in backdoor admin account now used in attacks.\nCSLU is a Windows app for managing licenses and linked products on-premises without connecting them to Cisco's cloud-based Smart Software Manager solution.\nCisco\npatched this security flaw\n(CVE-2024-20439) in September, describing it as \"an undocumented static user credential for an administrative account\" that lets unauthenticated attackers log into unpatched systems remotely with admin privileges over the Cisco Smart Licensing Utility (CSLU) app's API.\nCVE-2024-20439 only impacts systems running vulnerable Cisco Smart Licensing Utility releases, but it's only exploitable if the user starts the CSLU app (which doesn't run in the background by default).\nAruba threat researcher Nicholas Starke reverse-engineered the vulnerability two weeks after Cisco released security patches and\npublished a write-up\nwith technical details (including the decoded hardcoded static password).\n\"In March 2025, the Cisco Product Security Incident Response Team (PSIRT) became aware of attempted exploitation of this vulnerability in the wild,\" the company\nsaid in a Tuesday update\nto the original security advisory. \"Cisco continues to strongly recommend that customers upgrade to a fixed software release to remediate this vulnerability.\"\nChained with a second vulnerability\nWhile Cisco didn't share any details on these attacks, Johannes Ullrich, SANS Technology Institute's Dean of Research, spotted a campaign last month that used the backdoor admin account to attack CSLU instances exposed online.\nUllrich\nsaid in March\nthat threat actors are chaining CVE-2024-20439 with a second flaw, a critical CLSU information disclosure vulnerability (CVE-2024-20440) that unauthenticated attackers can exploit to gain access to log files containing sensitive data (including API credentials) by sending crafted HTTP requests to vulnerable devices.\n\"A quick search didn't show any active exploitation [at the time], but details, including the backdoor credentials, were published in a blog by Nicholas Starke shortly after Cisco released its advisory. So it is no surprise that we are seeing some exploit activity,\" Ullrich said.\nOn Monday, CISA added the CVE-2024-20439 static credential vulnerability to its\nKnown Exploited Vulnerabilities Catalog\n, ordering U.S. federal agencies to secure their systems against active exploitation within three weeks, by April 21.\nThis isn't the first backdoor account removed from Cisco products in recent years, with previous hardcoded credentials found in its\nIOS XE\n,\nWide Area Application Services (WAAS)\n,\nDigital Network Architecture (DNA) Center\n, and\nEmergency Responder\nsoftware.\nTop 10 MITRE ATT&CK\n©\nTechniques Behind 93% of Attacks\nBased on an analysis of 14M malicious actions, discover the top 10 MITRE ATT&CK techniques behind 93% of attacks and how to defend against them.\nRead the Red Report 2025\nRelated Articles:\nCritical Cisco Smart Licensing Utility flaws now exploited in attacks\nCISA tags Windows, Cisco vulnerabilities as actively exploited\nNew Windows zero-day exploited by 11 state hacking groups since 2017\nMicrosoft patches Windows Kernel zero-day exploited since 2023\nCritical PHP RCE vulnerability mass exploited in new attacks"
    },
    {
        "Title": "DPRK 'IT Workers' Pivot to Europe for Employment Scams",
        "Link": "https://www.darkreading.com/threat-intelligence/dprk-it-workers-europe-employment",
        "Summary": "By using fake references and building connections with recruiters, some North Korean nationals are landing six-figure jobs that replenish DPRK coffers.",
        "Date": "2025-04-02",
        "Content": "Error fetching article: 403 Client Error: Forbidden for url: https://www.darkreading.com/threat-intelligence/dprk-it-workers-europe-employment"
    },
    {
        "Title": "SolarWinds Adds Incident Management Tool From Squadcast",
        "Link": "https://www.darkreading.com/cyberattacks-data-breaches/solarwinds-adds-incident-management-tool-from-squadcast",
        "Summary": "The IT service management and observability tools company acquired Squadcast last month and is adding the automated incident response platform to the SolarWinds portfolio.",
        "Date": "2025-04-02",
        "Content": "Error fetching article: 403 Client Error: Forbidden for url: https://www.darkreading.com/cyberattacks-data-breaches/solarwinds-adds-incident-management-tool-from-squadcast"
    },
    {
        "Title": "In Salt Typhoon's Wake, Congress Mulls Potential Options",
        "Link": "https://www.darkreading.com/cyberattacks-data-breaches/salt-typhoons-wake-congress-potential-options",
        "Summary": "While the House Committee on Government Reform was looking for retaliatory options, cybersecurity experts pointed them toward building better defenses.",
        "Date": "2025-04-02",
        "Content": "Error fetching article: 403 Client Error: Forbidden for url: https://www.darkreading.com/cyberattacks-data-breaches/salt-typhoons-wake-congress-potential-options"
    },
    {
        "Title": "New PCI DSS Rules Say Merchants on Hook for Compliance, Not Providers",
        "Link": "https://www.darkreading.com/cyber-risk/new-pci-dss-rules-merchants-on-hook-compliance",
        "Summary": "Merchants and retailers will now face penalties for not being compliant with PCI DSS 4.0.1, and the increased security standards make it clear they cannot transfer compliance responsibility to third-party service providers.",
        "Date": "2025-04-02",
        "Content": "Error fetching article: 403 Client Error: Forbidden for url: https://www.darkreading.com/cyber-risk/new-pci-dss-rules-merchants-on-hook-compliance"
    },
    {
        "Title": "How an Interdiction Mindset Can Help Win War on Cyberattacks",
        "Link": "https://www.darkreading.com/cyberattacks-data-breaches/how-interdiction-mindset-cyberattacks",
        "Summary": "The US military and law enforcement learned to outthink insurgents. It's time for cybersecurity to learn to outsmart and outmaneuver threat actors with the same framework.",
        "Date": "2025-04-02",
        "Content": "Error fetching article: 403 Client Error: Forbidden for url: https://www.darkreading.com/cyberattacks-data-breaches/how-interdiction-mindset-cyberattacks"
    },
    {
        "Title": "Gootloader Malware Resurfaces in Google Ads for Legal Docs",
        "Link": "https://www.darkreading.com/cyberattacks-data-breaches/gootloader-malware-google-ads-legal-docs",
        "Summary": "Attackers target a familiar industry, law professionals, by hiding the infostealer in ads delivered via Google-based malvertising.",
        "Date": "2025-04-02",
        "Content": "Error fetching article: 403 Client Error: Forbidden for url: https://www.darkreading.com/cyberattacks-data-breaches/gootloader-malware-google-ads-legal-docs"
    },
    {
        "Title": "Malaysian Airport's Cyber Disruption a Warning for Asia",
        "Link": "https://www.darkreading.com/cyberattacks-data-breaches/malaysian-airport-cyber-disruption-warning-asia",
        "Summary": "Transportation facilities and networks slowly adapt to changes and threats, leaving them vulnerable to agile cyberattackers, as demonstrated by the $10 million ransomware attack.",
        "Date": "2025-04-02",
        "Content": "Error fetching article: 403 Client Error: Forbidden for url: https://www.darkreading.com/cyberattacks-data-breaches/malaysian-airport-cyber-disruption-warning-asia"
    },
    {
        "Title": "Serial Entrepreneurs Raise $43M to Counter AI Deepfakes, Social Engineering",
        "Link": "https://www.securityweek.com/serial-entrepreneurs-raise-43m-to-counter-ai-deepfakes-social-engineering/",
        "Summary": "<p>Adaptive is pitching a security platform designed to replicate real-world attack scenarios through AI-generated deepfake simulations.  </p>\n<p>The post <a href=\"https://www.securityweek.com/serial-entrepreneurs-raise-43m-to-counter-ai-deepfakes-social-engineering/\">Serial Entrepreneurs Raise $43M to Counter AI Deepfakes, Social Engineering</a> appeared first on <a href=\"https://www.securityweek.com\">SecurityWeek</a>.</p>",
        "Date": "2025-04-02",
        "Content": "Hi, what are you looking for?\nAdaptive is pitching a security platform designed to replicate real-world attack scenarios through AI-generated deepfake simulations.  \nBy\nFlipboard\nReddit\nWhatsapp\nWhatsapp\nEmail\nA list of prominent investors including Andreessen Horowitz (a16z) and the OpenAI Startup Fund have poured $43 million into Adaptive Security, a new startup promising technology to counter the surge in deepfake social engineering and AI-powered threats.\nThe startup, founded by serial entrepreneurs Brian Long and Andrew Jones, is building a security platform designed to replicate real-world attack scenarios through AI-generated deepfake simulations.  The pitch is to provide businesses with a testing ground to beef up internal human defenses. \n\n\n\nThe $43 million early-stage round, co-led by a16z and OpenAI’s venture capital arm, also included equity stakes for Abstract Ventures, Eniac Ventures, CrossBeam Ventures and K5.\n\n\n\nThe Abstract deal is OpenAI’s first-ever investment in a cybersecurity company. The funding round also included \n\n\n\nLong and Jones, whose track records include exits at TapCommerce and Attentive, is betting there’s a big enterprise market for security tooling to thwart sophisticated, AI-generated personas that can impersonate executives or employees and launch multi-pronged attacks across corporate communication channels.\n\n\n\n“With the right models and data, we can simulate realistic AI attacks, train employees to recognize threats, triage suspicious behavior in real time, and surface risk before it turns into loss,” the company said in a note announcing the capital raise.\n\n\n\nAt its core, Adaptive is pitching a platform to manage AI-powered deepfake attack simulations, real-time threat triage, next generation security training, and AI-driven risk scoring to corporate defenders.\n\n\n\nThe company said the product is capable of simulating real-world attacks using AI-generated personas across voice, SMS, and email. Much like existing security awareness programs delivered via video, the idea is test employees against deepfake social engineering lures.Advertisement. Scroll to continue reading.\n\n\n“If they fall for it, our system flags the risk instantly and delivers personalized training on the spot,” the company said. \n\n\n\nThe Adaptive platform is also promising real time threat triaging when someone at a company reports a suspicious message.  “Our AI doesn’t just forward it to IT, it analyzes the message in real time, scores the risk, and helps security teams act fast.”\n\n\n\nThe company said the platform also includes generative-AI tools for businesses to create tailored content in seconds (text, visuals and video) based on any topic or internal policy.  \n\n\n\nRelated: Bureau Raises $30M to Tackle Deepfakes, Payment Fraud\nRelated: Inside a Hacker’s Playbook – How Cybercriminals Use Deepfakes\nRelated: Surf Security Adds Deepfake Detection Tool to Enterprise Browser\nRelated: Reality Defender Banks $33M to Tackle AI-Generated Deepfakes\n\nThe startup, founded by serial entrepreneurs Brian Long and Andrew Jones, is building a security platform designed to replicate real-world attack scenarios through AI-generated deepfake simulations.  The pitch is to provide businesses with a testing ground to beef up internal human defenses. \n\n\n\nThe $43 million early-stage round, co-led by a16z and OpenAI’s venture capital arm, also included equity stakes for Abstract Ventures, Eniac Ventures, CrossBeam Ventures and K5.\n\n\n\nThe Abstract deal is OpenAI’s first-ever investment in a cybersecurity company. The funding round also included \n\n\n\nLong and Jones, whose track records include exits at TapCommerce and Attentive, is betting there’s a big enterprise market for security tooling to thwart sophisticated, AI-generated personas that can impersonate executives or employees and launch multi-pronged attacks across corporate communication channels.\n\n\n\n“With the right models and data, we can simulate realistic AI attacks, train employees to recognize threats, triage suspicious behavior in real time, and surface risk before it turns into loss,” the company said in a note announcing the capital raise.\n\n\n\nAt its core, Adaptive is pitching a platform to manage AI-powered deepfake attack simulations, real-time threat triage, next generation security training, and AI-driven risk scoring to corporate defenders.\n\n\n\nThe company said the product is capable of simulating real-world attacks using AI-generated personas across voice, SMS, and email. Much like existing security awareness programs delivered via video, the idea is test employees against deepfake social engineering lures.Advertisement. Scroll to continue reading.\n\n\n“If they fall for it, our system flags the risk instantly and delivers personalized training on the spot,” the company said. \n\n\n\nThe Adaptive platform is also promising real time threat triaging when someone at a company reports a suspicious message.  “Our AI doesn’t just forward it to IT, it analyzes the message in real time, scores the risk, and helps security teams act fast.”\n\n\n\nThe company said the platform also includes generative-AI tools for businesses to create tailored content in seconds (text, visuals and video) based on any topic or internal policy.  \n\n\n\nRelated: Bureau Raises $30M to Tackle Deepfakes, Payment Fraud\nRelated: Inside a Hacker’s Playbook – How Cybercriminals Use Deepfakes\nRelated: Surf Security Adds Deepfake Detection Tool to Enterprise Browser\nRelated: Reality Defender Banks $33M to Tackle AI-Generated Deepfakes\n\nThe $43 million early-stage round, co-led by a16z and OpenAI’s venture capital arm, also included equity stakes for Abstract Ventures, Eniac Ventures, CrossBeam Ventures and K5.\n\n\n\nThe Abstract deal is OpenAI’s first-ever investment in a cybersecurity company. The funding round also included \n\n\n\nLong and Jones, whose track records include exits at TapCommerce and Attentive, is betting there’s a big enterprise market for security tooling to thwart sophisticated, AI-generated personas that can impersonate executives or employees and launch multi-pronged attacks across corporate communication channels.\n\n\n\n“With the right models and data, we can simulate realistic AI attacks, train employees to recognize threats, triage suspicious behavior in real time, and surface risk before it turns into loss,” the company said in a note announcing the capital raise.\n\n\n\nAt its core, Adaptive is pitching a platform to manage AI-powered deepfake attack simulations, real-time threat triage, next generation security training, and AI-driven risk scoring to corporate defenders.\n\n\n\nThe company said the product is capable of simulating real-world attacks using AI-generated personas across voice, SMS, and email. Much like existing security awareness programs delivered via video, the idea is test employees against deepfake social engineering lures.Advertisement. Scroll to continue reading.\n\n\n“If they fall for it, our system flags the risk instantly and delivers personalized training on the spot,” the company said. \n\n\n\nThe Adaptive platform is also promising real time threat triaging when someone at a company reports a suspicious message.  “Our AI doesn’t just forward it to IT, it analyzes the message in real time, scores the risk, and helps security teams act fast.”\n\n\n\nThe company said the platform also includes generative-AI tools for businesses to create tailored content in seconds (text, visuals and video) based on any topic or internal policy.  \n\n\n\nRelated: Bureau Raises $30M to Tackle Deepfakes, Payment Fraud\nRelated: Inside a Hacker’s Playbook – How Cybercriminals Use Deepfakes\nRelated: Surf Security Adds Deepfake Detection Tool to Enterprise Browser\nRelated: Reality Defender Banks $33M to Tackle AI-Generated Deepfakes\n\nThe Abstract deal is OpenAI’s first-ever investment in a cybersecurity company. The funding round also included \n\n\n\nLong and Jones, whose track records include exits at TapCommerce and Attentive, is betting there’s a big enterprise market for security tooling to thwart sophisticated, AI-generated personas that can impersonate executives or employees and launch multi-pronged attacks across corporate communication channels.\n\n\n\n“With the right models and data, we can simulate realistic AI attacks, train employees to recognize threats, triage suspicious behavior in real time, and surface risk before it turns into loss,” the company said in a note announcing the capital raise.\n\n\n\nAt its core, Adaptive is pitching a platform to manage AI-powered deepfake attack simulations, real-time threat triage, next generation security training, and AI-driven risk scoring to corporate defenders.\n\n\n\nThe company said the product is capable of simulating real-world attacks using AI-generated personas across voice, SMS, and email. Much like existing security awareness programs delivered via video, the idea is test employees against deepfake social engineering lures.Advertisement. Scroll to continue reading.\n\n\n“If they fall for it, our system flags the risk instantly and delivers personalized training on the spot,” the company said. \n\n\n\nThe Adaptive platform is also promising real time threat triaging when someone at a company reports a suspicious message.  “Our AI doesn’t just forward it to IT, it analyzes the message in real time, scores the risk, and helps security teams act fast.”\n\n\n\nThe company said the platform also includes generative-AI tools for businesses to create tailored content in seconds (text, visuals and video) based on any topic or internal policy.  \n\n\n\nRelated: Bureau Raises $30M to Tackle Deepfakes, Payment Fraud\nRelated: Inside a Hacker’s Playbook – How Cybercriminals Use Deepfakes\nRelated: Surf Security Adds Deepfake Detection Tool to Enterprise Browser\nRelated: Reality Defender Banks $33M to Tackle AI-Generated Deepfakes\n\nLong and Jones, whose track records include exits at TapCommerce and Attentive, is betting there’s a big enterprise market for security tooling to thwart sophisticated, AI-generated personas that can impersonate executives or employees and launch multi-pronged attacks across corporate communication channels.\n\n\n\n“With the right models and data, we can simulate realistic AI attacks, train employees to recognize threats, triage suspicious behavior in real time, and surface risk before it turns into loss,” the company said in a note announcing the capital raise.\n\n\n\nAt its core, Adaptive is pitching a platform to manage AI-powered deepfake attack simulations, real-time threat triage, next generation security training, and AI-driven risk scoring to corporate defenders.\n\n\n\nThe company said the product is capable of simulating real-world attacks using AI-generated personas across voice, SMS, and email. Much like existing security awareness programs delivered via video, the idea is test employees against deepfake social engineering lures.Advertisement. Scroll to continue reading.\n\n\n“If they fall for it, our system flags the risk instantly and delivers personalized training on the spot,” the company said. \n\n\n\nThe Adaptive platform is also promising real time threat triaging when someone at a company reports a suspicious message.  “Our AI doesn’t just forward it to IT, it analyzes the message in real time, scores the risk, and helps security teams act fast.”\n\n\n\nThe company said the platform also includes generative-AI tools for businesses to create tailored content in seconds (text, visuals and video) based on any topic or internal policy.  \n\n\n\nRelated: Bureau Raises $30M to Tackle Deepfakes, Payment Fraud\nRelated: Inside a Hacker’s Playbook – How Cybercriminals Use Deepfakes\nRelated: Surf Security Adds Deepfake Detection Tool to Enterprise Browser\nRelated: Reality Defender Banks $33M to Tackle AI-Generated Deepfakes\n\n“With the right models and data, we can simulate realistic AI attacks, train employees to recognize threats, triage suspicious behavior in real time, and surface risk before it turns into loss,” the company said in a note announcing the capital raise.\n\n\n\nAt its core, Adaptive is pitching a platform to manage AI-powered deepfake attack simulations, real-time threat triage, next generation security training, and AI-driven risk scoring to corporate defenders.\n\n\n\nThe company said the product is capable of simulating real-world attacks using AI-generated personas across voice, SMS, and email. Much like existing security awareness programs delivered via video, the idea is test employees against deepfake social engineering lures.Advertisement. Scroll to continue reading.\n\n\n“If they fall for it, our system flags the risk instantly and delivers personalized training on the spot,” the company said. \n\n\n\nThe Adaptive platform is also promising real time threat triaging when someone at a company reports a suspicious message.  “Our AI doesn’t just forward it to IT, it analyzes the message in real time, scores the risk, and helps security teams act fast.”\n\n\n\nThe company said the platform also includes generative-AI tools for businesses to create tailored content in seconds (text, visuals and video) based on any topic or internal policy.  \n\n\n\nRelated: Bureau Raises $30M to Tackle Deepfakes, Payment Fraud\nRelated: Inside a Hacker’s Playbook – How Cybercriminals Use Deepfakes\nRelated: Surf Security Adds Deepfake Detection Tool to Enterprise Browser\nRelated: Reality Defender Banks $33M to Tackle AI-Generated Deepfakes\n\nAt its core, Adaptive is pitching a platform to manage AI-powered deepfake attack simulations, real-time threat triage, next generation security training, and AI-driven risk scoring to corporate defenders.\n\n\n\nThe company said the product is capable of simulating real-world attacks using AI-generated personas across voice, SMS, and email. Much like existing security awareness programs delivered via video, the idea is test employees against deepfake social engineering lures.Advertisement. Scroll to continue reading.\n\n\n“If they fall for it, our system flags the risk instantly and delivers personalized training on the spot,” the company said. \n\n\n\nThe Adaptive platform is also promising real time threat triaging when someone at a company reports a suspicious message.  “Our AI doesn’t just forward it to IT, it analyzes the message in real time, scores the risk, and helps security teams act fast.”\n\n\n\nThe company said the platform also includes generative-AI tools for businesses to create tailored content in seconds (text, visuals and video) based on any topic or internal policy.  \n\n\n\nRelated: Bureau Raises $30M to Tackle Deepfakes, Payment Fraud\nRelated: Inside a Hacker’s Playbook – How Cybercriminals Use Deepfakes\nRelated: Surf Security Adds Deepfake Detection Tool to Enterprise Browser\nRelated: Reality Defender Banks $33M to Tackle AI-Generated Deepfakes\n\nThe company said the product is capable of simulating real-world attacks using AI-generated personas across voice, SMS, and email. Much like existing security awareness programs delivered via video, the idea is test employees against deepfake social engineering lures.Advertisement. Scroll to continue reading.\n\n\n“If they fall for it, our system flags the risk instantly and delivers personalized training on the spot,” the company said. \n\n\n\nThe Adaptive platform is also promising real time threat triaging when someone at a company reports a suspicious message.  “Our AI doesn’t just forward it to IT, it analyzes the message in real time, scores the risk, and helps security teams act fast.”\n\n\n\nThe company said the platform also includes generative-AI tools for businesses to create tailored content in seconds (text, visuals and video) based on any topic or internal policy.  \n\n\n\nRelated: Bureau Raises $30M to Tackle Deepfakes, Payment Fraud\nRelated: Inside a Hacker’s Playbook – How Cybercriminals Use Deepfakes\nRelated: Surf Security Adds Deepfake Detection Tool to Enterprise Browser\nRelated: Reality Defender Banks $33M to Tackle AI-Generated Deepfakes\n\n“If they fall for it, our system flags the risk instantly and delivers personalized training on the spot,” the company said. \n\n\n\nThe Adaptive platform is also promising real time threat triaging when someone at a company reports a suspicious message.  “Our AI doesn’t just forward it to IT, it analyzes the message in real time, scores the risk, and helps security teams act fast.”\n\n\n\nThe company said the platform also includes generative-AI tools for businesses to create tailored content in seconds (text, visuals and video) based on any topic or internal policy.  \n\n\n\nRelated: Bureau Raises $30M to Tackle Deepfakes, Payment Fraud\nRelated: Inside a Hacker’s Playbook – How Cybercriminals Use Deepfakes\nRelated: Surf Security Adds Deepfake Detection Tool to Enterprise Browser\nRelated: Reality Defender Banks $33M to Tackle AI-Generated Deepfakes\n\nThe Adaptive platform is also promising real time threat triaging when someone at a company reports a suspicious message.  “Our AI doesn’t just forward it to IT, it analyzes the message in real time, scores the risk, and helps security teams act fast.”\n\n\n\nThe company said the platform also includes generative-AI tools for businesses to create tailored content in seconds (text, visuals and video) based on any topic or internal policy.  \n\n\n\nRelated: Bureau Raises $30M to Tackle Deepfakes, Payment Fraud\nRelated: Inside a Hacker’s Playbook – How Cybercriminals Use Deepfakes\nRelated: Surf Security Adds Deepfake Detection Tool to Enterprise Browser\nRelated: Reality Defender Banks $33M to Tackle AI-Generated Deepfakes\n\nThe company said the platform also includes generative-AI tools for businesses to create tailored content in seconds (text, visuals and video) based on any topic or internal policy.  \n\n\n\nRelated: Bureau Raises $30M to Tackle Deepfakes, Payment Fraud\nRelated: Inside a Hacker’s Playbook – How Cybercriminals Use Deepfakes\nRelated: Surf Security Adds Deepfake Detection Tool to Enterprise Browser\nRelated: Reality Defender Banks $33M to Tackle AI-Generated Deepfakes\n\nRelated: Bureau Raises $30M to Tackle Deepfakes, Payment Fraud\nRelated: Inside a Hacker’s Playbook – How Cybercriminals Use Deepfakes\nRelated: Surf Security Adds Deepfake Detection Tool to Enterprise Browser\nRelated: Reality Defender Banks $33M to Tackle AI-Generated Deepfakes\n\nRelated: Inside a Hacker’s Playbook – How Cybercriminals Use Deepfakes\nRelated: Surf Security Adds Deepfake Detection Tool to Enterprise Browser\nRelated: Reality Defender Banks $33M to Tackle AI-Generated Deepfakes\n\nRelated: Surf Security Adds Deepfake Detection Tool to Enterprise Browser\nRelated: Reality Defender Banks $33M to Tackle AI-Generated Deepfakes\n\nRelated: Reality Defender Banks $33M to Tackle AI-Generated Deepfakes\n\nRyan Naraine is Editor-at-Large at SecurityWeek and host of the popular Security Conversations podcast series. He is a security community engagement expert who has built programs at major global brands, including Intel Corp., Bishop Fox and GReAT.  Ryan is a founding-director of the Security Tinkerers non-profit, an advisor to early-stage entrepreneurs, and a regular speaker at security conferences around the world.\nSubscribe to the SecurityWeek Email Briefing to stay informed on the latest threats, trends, and technology, along with insightful columns from industry experts.\nJoin this event as we dive into threat hunting tools and frameworks, and explore value of threat intelligence data in the defender’s security stack.\nLearn how integrating BAS and Automated Penetration Testing empowers security teams to quickly identify and validate threats, enabling prompt response and remediation.\nIdentity verification and fraud prevention firm Trulioo has appointed Vicky Bindra as CEO.\nCommvault has appointed Bill O'Connell as its Chief Security Officer.\nKevin Paige has been appointed Field CISO at identity governance company ConductorOne.\n\n\nThe rise of zero-knowledge threat actors powered by AI marks a turning point in the business of cybercrime where sophisticated attacks are no longer confined to skilled attackers.\n(Etay Maor)\n\n\n\nA strong security program will sometimes require substantial organizational and cultural changes around security practices, and inevitably, a higher cost.\n(Trevin Edgeworth)\n\n\n\nInterview with Taylor Pyle, a Cybersecurity Engineer at Viasat on her experience with both cyber and mentorship.\n(Marc Solomon)\n\n\n\nA Joni Mitchell song from the 1960s can teach us a lot about securing hybrid and multi-cloud environments.\n(Joshua Goldfarb)\n\n\n\nDefending high profile sporting events from adversarial attacks requires a mix of experienced capabilities and a solid threat intelligence program.\n(Marc Solomon)\n\nFlipboard\nReddit\nWhatsapp\nWhatsapp\nEmail\nGot a confidential news tip? We want to hear from you.\nReach a large audience of enterprise cybersecurity professionals \nSubscribe to the SecurityWeek Daily Briefing and get the latest content delivered to your inbox.\nCopyright © 2025 SecurityWeek ®, a Wired Business Media Publication. All Rights Reserved. \n\r\n\t\t\t\t\tSubscribe to the SecurityWeek Email Briefing to stay informed on the latest cybersecurity\r\n\t\t\t\t\tnews, threats, and expert insights. Unsubscribe at any time.\r\n\t\t\t\t\t\n\n\n"
    },
    {
        "Title": "Vulnerabilities Expose Jan AI Systems to Remote Manipulation",
        "Link": "https://www.securityweek.com/vulnerabilities-expose-jan-ai-systems-to-remote-manipulation/",
        "Summary": "<p>Vulnerabilities in open source ChatGPT alternative Jan AI expose systems to remote, unauthenticated manipulation.</p>\n<p>The post <a href=\"https://www.securityweek.com/vulnerabilities-expose-jan-ai-systems-to-remote-manipulation/\">Vulnerabilities Expose Jan AI Systems to Remote Manipulation</a> appeared first on <a href=\"https://www.securityweek.com\">SecurityWeek</a>.</p>",
        "Date": "2025-04-02",
        "Content": "Hi, what are you looking for?\nVulnerabilities in open source ChatGPT alternative Jan AI expose systems to remote, unauthenticated manipulation.\nBy\nFlipboard\nReddit\nWhatsapp\nWhatsapp\nEmail\nMultiple vulnerabilities in Jan AI, which is advertised as an open source ChatGPT alternative, could be exploited by remote, unauthenticated attackers to manipulate systems, developer security platform Snyk warns.\nDeveloped by Menlo Research, Jan AI is a personal assistant that runs offline on desktops and mobile devices, featuring a model library with popular LLMs, and support for extensions for customization purposes.\n\n\n\nJan, which has over one million downloads on GitHub, allows users to download and run LLMs locally, without depending on cloud hosting services and benefiting from full control over the AI.\n\n\n\nThe assistant is powered by Menlo’s self-hosted AI engine Cortex.cpp, which functions as the backend API server, and has an Electron application as a user interface. Through Cortex, users can pull models from a dedicated hub and from HuggingFace, and can import local models stored in the GGUF file format.\n\n\n\nBecause Jan and Cortex are meant to operate locally, they lack authentication, meaning that they are prone to attacks originating from malicious webpages.\n\n\n\nSnyk’s analysis of the AI assistant uncovered a function for uploading files to the server that lacked sanitization, which could be exploited by a malicious page to write arbitrary files to the machine.\n\n\n\nFurther investigation revealed out-of-bound issues in Jan’s GGUF parser, as well as the lack of cross-site request forgery (CSRF) protections on its server, which could be exploited on non-GET endpoints, despite Cortex having cross-origin resource sharing (CORS) implemented.\n\n\n\nBy exploiting the cross-origin arbitrary file write flaw, an attacker could write a crafted GGUF file to the server, and then exploit the lack of CSRF protection to import it and trigger an out-of-bounds read that enables the attacker to read data into a metadata field they control.Advertisement. Scroll to continue reading.\n\n\nBy sending a cross-origin request, the attacker can update the server’s configuration and completely disable CORS, and then read back the leaked data by sending a request to the model’s metadata endpoint, Snyk says.\n\n\n\n“Leaking data over the network with a GGUF file is pretty neat, but this doesn’t come without some limitations. We can’t control what gets mapped after our crafted model file; hence there’s no way to tell if we can leak sensitive data,” the security firm notes.\n\n\n\nThe AI was also found vulnerable to remote code execution (RCE), through Cortex.cpp’s support for python-engine. Because the engine is a C++ wrapper that executes the Python binary, an attacker can update the model configuration to inject a payload in the binary and trigger command execution when the model starts.\n\n\n\nSnyk reported its findings to Menlo on February 18 and all issues were addressed by March 6. Four CVEs were issued: CVE-2025-2446 (arbitrary file write via path traversal), CVE-2025-2439 (out-of-bound read in GGUF parser), CVE-2025-2445 (command injection in Python engine model update), and CVE-2025-2447 (missing CSRF protection).\n\n\n\nRelated: New AI Security Tool Helps Organizations Set Trust Zones for Gen-AI Models\nRelated: New Jailbreak Technique Uses Fictional World to Manipulate AI\nRelated: New CCA Jailbreak Method Works Against Most AI Models\nRelated: Prompt Security Raises $18 Million for Gen-AI Security Platform\n\nDeveloped by Menlo Research, Jan AI is a personal assistant that runs offline on desktops and mobile devices, featuring a model library with popular LLMs, and support for extensions for customization purposes.\n\n\n\nJan, which has over one million downloads on GitHub, allows users to download and run LLMs locally, without depending on cloud hosting services and benefiting from full control over the AI.\n\n\n\nThe assistant is powered by Menlo’s self-hosted AI engine Cortex.cpp, which functions as the backend API server, and has an Electron application as a user interface. Through Cortex, users can pull models from a dedicated hub and from HuggingFace, and can import local models stored in the GGUF file format.\n\n\n\nBecause Jan and Cortex are meant to operate locally, they lack authentication, meaning that they are prone to attacks originating from malicious webpages.\n\n\n\nSnyk’s analysis of the AI assistant uncovered a function for uploading files to the server that lacked sanitization, which could be exploited by a malicious page to write arbitrary files to the machine.\n\n\n\nFurther investigation revealed out-of-bound issues in Jan’s GGUF parser, as well as the lack of cross-site request forgery (CSRF) protections on its server, which could be exploited on non-GET endpoints, despite Cortex having cross-origin resource sharing (CORS) implemented.\n\n\n\nBy exploiting the cross-origin arbitrary file write flaw, an attacker could write a crafted GGUF file to the server, and then exploit the lack of CSRF protection to import it and trigger an out-of-bounds read that enables the attacker to read data into a metadata field they control.Advertisement. Scroll to continue reading.\n\n\nBy sending a cross-origin request, the attacker can update the server’s configuration and completely disable CORS, and then read back the leaked data by sending a request to the model’s metadata endpoint, Snyk says.\n\n\n\n“Leaking data over the network with a GGUF file is pretty neat, but this doesn’t come without some limitations. We can’t control what gets mapped after our crafted model file; hence there’s no way to tell if we can leak sensitive data,” the security firm notes.\n\n\n\nThe AI was also found vulnerable to remote code execution (RCE), through Cortex.cpp’s support for python-engine. Because the engine is a C++ wrapper that executes the Python binary, an attacker can update the model configuration to inject a payload in the binary and trigger command execution when the model starts.\n\n\n\nSnyk reported its findings to Menlo on February 18 and all issues were addressed by March 6. Four CVEs were issued: CVE-2025-2446 (arbitrary file write via path traversal), CVE-2025-2439 (out-of-bound read in GGUF parser), CVE-2025-2445 (command injection in Python engine model update), and CVE-2025-2447 (missing CSRF protection).\n\n\n\nRelated: New AI Security Tool Helps Organizations Set Trust Zones for Gen-AI Models\nRelated: New Jailbreak Technique Uses Fictional World to Manipulate AI\nRelated: New CCA Jailbreak Method Works Against Most AI Models\nRelated: Prompt Security Raises $18 Million for Gen-AI Security Platform\n\nJan, which has over one million downloads on GitHub, allows users to download and run LLMs locally, without depending on cloud hosting services and benefiting from full control over the AI.\n\n\n\nThe assistant is powered by Menlo’s self-hosted AI engine Cortex.cpp, which functions as the backend API server, and has an Electron application as a user interface. Through Cortex, users can pull models from a dedicated hub and from HuggingFace, and can import local models stored in the GGUF file format.\n\n\n\nBecause Jan and Cortex are meant to operate locally, they lack authentication, meaning that they are prone to attacks originating from malicious webpages.\n\n\n\nSnyk’s analysis of the AI assistant uncovered a function for uploading files to the server that lacked sanitization, which could be exploited by a malicious page to write arbitrary files to the machine.\n\n\n\nFurther investigation revealed out-of-bound issues in Jan’s GGUF parser, as well as the lack of cross-site request forgery (CSRF) protections on its server, which could be exploited on non-GET endpoints, despite Cortex having cross-origin resource sharing (CORS) implemented.\n\n\n\nBy exploiting the cross-origin arbitrary file write flaw, an attacker could write a crafted GGUF file to the server, and then exploit the lack of CSRF protection to import it and trigger an out-of-bounds read that enables the attacker to read data into a metadata field they control.Advertisement. Scroll to continue reading.\n\n\nBy sending a cross-origin request, the attacker can update the server’s configuration and completely disable CORS, and then read back the leaked data by sending a request to the model’s metadata endpoint, Snyk says.\n\n\n\n“Leaking data over the network with a GGUF file is pretty neat, but this doesn’t come without some limitations. We can’t control what gets mapped after our crafted model file; hence there’s no way to tell if we can leak sensitive data,” the security firm notes.\n\n\n\nThe AI was also found vulnerable to remote code execution (RCE), through Cortex.cpp’s support for python-engine. Because the engine is a C++ wrapper that executes the Python binary, an attacker can update the model configuration to inject a payload in the binary and trigger command execution when the model starts.\n\n\n\nSnyk reported its findings to Menlo on February 18 and all issues were addressed by March 6. Four CVEs were issued: CVE-2025-2446 (arbitrary file write via path traversal), CVE-2025-2439 (out-of-bound read in GGUF parser), CVE-2025-2445 (command injection in Python engine model update), and CVE-2025-2447 (missing CSRF protection).\n\n\n\nRelated: New AI Security Tool Helps Organizations Set Trust Zones for Gen-AI Models\nRelated: New Jailbreak Technique Uses Fictional World to Manipulate AI\nRelated: New CCA Jailbreak Method Works Against Most AI Models\nRelated: Prompt Security Raises $18 Million for Gen-AI Security Platform\n\nThe assistant is powered by Menlo’s self-hosted AI engine Cortex.cpp, which functions as the backend API server, and has an Electron application as a user interface. Through Cortex, users can pull models from a dedicated hub and from HuggingFace, and can import local models stored in the GGUF file format.\n\n\n\nBecause Jan and Cortex are meant to operate locally, they lack authentication, meaning that they are prone to attacks originating from malicious webpages.\n\n\n\nSnyk’s analysis of the AI assistant uncovered a function for uploading files to the server that lacked sanitization, which could be exploited by a malicious page to write arbitrary files to the machine.\n\n\n\nFurther investigation revealed out-of-bound issues in Jan’s GGUF parser, as well as the lack of cross-site request forgery (CSRF) protections on its server, which could be exploited on non-GET endpoints, despite Cortex having cross-origin resource sharing (CORS) implemented.\n\n\n\nBy exploiting the cross-origin arbitrary file write flaw, an attacker could write a crafted GGUF file to the server, and then exploit the lack of CSRF protection to import it and trigger an out-of-bounds read that enables the attacker to read data into a metadata field they control.Advertisement. Scroll to continue reading.\n\n\nBy sending a cross-origin request, the attacker can update the server’s configuration and completely disable CORS, and then read back the leaked data by sending a request to the model’s metadata endpoint, Snyk says.\n\n\n\n“Leaking data over the network with a GGUF file is pretty neat, but this doesn’t come without some limitations. We can’t control what gets mapped after our crafted model file; hence there’s no way to tell if we can leak sensitive data,” the security firm notes.\n\n\n\nThe AI was also found vulnerable to remote code execution (RCE), through Cortex.cpp’s support for python-engine. Because the engine is a C++ wrapper that executes the Python binary, an attacker can update the model configuration to inject a payload in the binary and trigger command execution when the model starts.\n\n\n\nSnyk reported its findings to Menlo on February 18 and all issues were addressed by March 6. Four CVEs were issued: CVE-2025-2446 (arbitrary file write via path traversal), CVE-2025-2439 (out-of-bound read in GGUF parser), CVE-2025-2445 (command injection in Python engine model update), and CVE-2025-2447 (missing CSRF protection).\n\n\n\nRelated: New AI Security Tool Helps Organizations Set Trust Zones for Gen-AI Models\nRelated: New Jailbreak Technique Uses Fictional World to Manipulate AI\nRelated: New CCA Jailbreak Method Works Against Most AI Models\nRelated: Prompt Security Raises $18 Million for Gen-AI Security Platform\n\nBecause Jan and Cortex are meant to operate locally, they lack authentication, meaning that they are prone to attacks originating from malicious webpages.\n\n\n\nSnyk’s analysis of the AI assistant uncovered a function for uploading files to the server that lacked sanitization, which could be exploited by a malicious page to write arbitrary files to the machine.\n\n\n\nFurther investigation revealed out-of-bound issues in Jan’s GGUF parser, as well as the lack of cross-site request forgery (CSRF) protections on its server, which could be exploited on non-GET endpoints, despite Cortex having cross-origin resource sharing (CORS) implemented.\n\n\n\nBy exploiting the cross-origin arbitrary file write flaw, an attacker could write a crafted GGUF file to the server, and then exploit the lack of CSRF protection to import it and trigger an out-of-bounds read that enables the attacker to read data into a metadata field they control.Advertisement. Scroll to continue reading.\n\n\nBy sending a cross-origin request, the attacker can update the server’s configuration and completely disable CORS, and then read back the leaked data by sending a request to the model’s metadata endpoint, Snyk says.\n\n\n\n“Leaking data over the network with a GGUF file is pretty neat, but this doesn’t come without some limitations. We can’t control what gets mapped after our crafted model file; hence there’s no way to tell if we can leak sensitive data,” the security firm notes.\n\n\n\nThe AI was also found vulnerable to remote code execution (RCE), through Cortex.cpp’s support for python-engine. Because the engine is a C++ wrapper that executes the Python binary, an attacker can update the model configuration to inject a payload in the binary and trigger command execution when the model starts.\n\n\n\nSnyk reported its findings to Menlo on February 18 and all issues were addressed by March 6. Four CVEs were issued: CVE-2025-2446 (arbitrary file write via path traversal), CVE-2025-2439 (out-of-bound read in GGUF parser), CVE-2025-2445 (command injection in Python engine model update), and CVE-2025-2447 (missing CSRF protection).\n\n\n\nRelated: New AI Security Tool Helps Organizations Set Trust Zones for Gen-AI Models\nRelated: New Jailbreak Technique Uses Fictional World to Manipulate AI\nRelated: New CCA Jailbreak Method Works Against Most AI Models\nRelated: Prompt Security Raises $18 Million for Gen-AI Security Platform\n\nSnyk’s analysis of the AI assistant uncovered a function for uploading files to the server that lacked sanitization, which could be exploited by a malicious page to write arbitrary files to the machine.\n\n\n\nFurther investigation revealed out-of-bound issues in Jan’s GGUF parser, as well as the lack of cross-site request forgery (CSRF) protections on its server, which could be exploited on non-GET endpoints, despite Cortex having cross-origin resource sharing (CORS) implemented.\n\n\n\nBy exploiting the cross-origin arbitrary file write flaw, an attacker could write a crafted GGUF file to the server, and then exploit the lack of CSRF protection to import it and trigger an out-of-bounds read that enables the attacker to read data into a metadata field they control.Advertisement. Scroll to continue reading.\n\n\nBy sending a cross-origin request, the attacker can update the server’s configuration and completely disable CORS, and then read back the leaked data by sending a request to the model’s metadata endpoint, Snyk says.\n\n\n\n“Leaking data over the network with a GGUF file is pretty neat, but this doesn’t come without some limitations. We can’t control what gets mapped after our crafted model file; hence there’s no way to tell if we can leak sensitive data,” the security firm notes.\n\n\n\nThe AI was also found vulnerable to remote code execution (RCE), through Cortex.cpp’s support for python-engine. Because the engine is a C++ wrapper that executes the Python binary, an attacker can update the model configuration to inject a payload in the binary and trigger command execution when the model starts.\n\n\n\nSnyk reported its findings to Menlo on February 18 and all issues were addressed by March 6. Four CVEs were issued: CVE-2025-2446 (arbitrary file write via path traversal), CVE-2025-2439 (out-of-bound read in GGUF parser), CVE-2025-2445 (command injection in Python engine model update), and CVE-2025-2447 (missing CSRF protection).\n\n\n\nRelated: New AI Security Tool Helps Organizations Set Trust Zones for Gen-AI Models\nRelated: New Jailbreak Technique Uses Fictional World to Manipulate AI\nRelated: New CCA Jailbreak Method Works Against Most AI Models\nRelated: Prompt Security Raises $18 Million for Gen-AI Security Platform\n\nFurther investigation revealed out-of-bound issues in Jan’s GGUF parser, as well as the lack of cross-site request forgery (CSRF) protections on its server, which could be exploited on non-GET endpoints, despite Cortex having cross-origin resource sharing (CORS) implemented.\n\n\n\nBy exploiting the cross-origin arbitrary file write flaw, an attacker could write a crafted GGUF file to the server, and then exploit the lack of CSRF protection to import it and trigger an out-of-bounds read that enables the attacker to read data into a metadata field they control.Advertisement. Scroll to continue reading.\n\n\nBy sending a cross-origin request, the attacker can update the server’s configuration and completely disable CORS, and then read back the leaked data by sending a request to the model’s metadata endpoint, Snyk says.\n\n\n\n“Leaking data over the network with a GGUF file is pretty neat, but this doesn’t come without some limitations. We can’t control what gets mapped after our crafted model file; hence there’s no way to tell if we can leak sensitive data,” the security firm notes.\n\n\n\nThe AI was also found vulnerable to remote code execution (RCE), through Cortex.cpp’s support for python-engine. Because the engine is a C++ wrapper that executes the Python binary, an attacker can update the model configuration to inject a payload in the binary and trigger command execution when the model starts.\n\n\n\nSnyk reported its findings to Menlo on February 18 and all issues were addressed by March 6. Four CVEs were issued: CVE-2025-2446 (arbitrary file write via path traversal), CVE-2025-2439 (out-of-bound read in GGUF parser), CVE-2025-2445 (command injection in Python engine model update), and CVE-2025-2447 (missing CSRF protection).\n\n\n\nRelated: New AI Security Tool Helps Organizations Set Trust Zones for Gen-AI Models\nRelated: New Jailbreak Technique Uses Fictional World to Manipulate AI\nRelated: New CCA Jailbreak Method Works Against Most AI Models\nRelated: Prompt Security Raises $18 Million for Gen-AI Security Platform\n\nBy exploiting the cross-origin arbitrary file write flaw, an attacker could write a crafted GGUF file to the server, and then exploit the lack of CSRF protection to import it and trigger an out-of-bounds read that enables the attacker to read data into a metadata field they control.Advertisement. Scroll to continue reading.\n\n\nBy sending a cross-origin request, the attacker can update the server’s configuration and completely disable CORS, and then read back the leaked data by sending a request to the model’s metadata endpoint, Snyk says.\n\n\n\n“Leaking data over the network with a GGUF file is pretty neat, but this doesn’t come without some limitations. We can’t control what gets mapped after our crafted model file; hence there’s no way to tell if we can leak sensitive data,” the security firm notes.\n\n\n\nThe AI was also found vulnerable to remote code execution (RCE), through Cortex.cpp’s support for python-engine. Because the engine is a C++ wrapper that executes the Python binary, an attacker can update the model configuration to inject a payload in the binary and trigger command execution when the model starts.\n\n\n\nSnyk reported its findings to Menlo on February 18 and all issues were addressed by March 6. Four CVEs were issued: CVE-2025-2446 (arbitrary file write via path traversal), CVE-2025-2439 (out-of-bound read in GGUF parser), CVE-2025-2445 (command injection in Python engine model update), and CVE-2025-2447 (missing CSRF protection).\n\n\n\nRelated: New AI Security Tool Helps Organizations Set Trust Zones for Gen-AI Models\nRelated: New Jailbreak Technique Uses Fictional World to Manipulate AI\nRelated: New CCA Jailbreak Method Works Against Most AI Models\nRelated: Prompt Security Raises $18 Million for Gen-AI Security Platform\n\nBy sending a cross-origin request, the attacker can update the server’s configuration and completely disable CORS, and then read back the leaked data by sending a request to the model’s metadata endpoint, Snyk says.\n\n\n\n“Leaking data over the network with a GGUF file is pretty neat, but this doesn’t come without some limitations. We can’t control what gets mapped after our crafted model file; hence there’s no way to tell if we can leak sensitive data,” the security firm notes.\n\n\n\nThe AI was also found vulnerable to remote code execution (RCE), through Cortex.cpp’s support for python-engine. Because the engine is a C++ wrapper that executes the Python binary, an attacker can update the model configuration to inject a payload in the binary and trigger command execution when the model starts.\n\n\n\nSnyk reported its findings to Menlo on February 18 and all issues were addressed by March 6. Four CVEs were issued: CVE-2025-2446 (arbitrary file write via path traversal), CVE-2025-2439 (out-of-bound read in GGUF parser), CVE-2025-2445 (command injection in Python engine model update), and CVE-2025-2447 (missing CSRF protection).\n\n\n\nRelated: New AI Security Tool Helps Organizations Set Trust Zones for Gen-AI Models\nRelated: New Jailbreak Technique Uses Fictional World to Manipulate AI\nRelated: New CCA Jailbreak Method Works Against Most AI Models\nRelated: Prompt Security Raises $18 Million for Gen-AI Security Platform\n\n“Leaking data over the network with a GGUF file is pretty neat, but this doesn’t come without some limitations. We can’t control what gets mapped after our crafted model file; hence there’s no way to tell if we can leak sensitive data,” the security firm notes.\n\n\n\nThe AI was also found vulnerable to remote code execution (RCE), through Cortex.cpp’s support for python-engine. Because the engine is a C++ wrapper that executes the Python binary, an attacker can update the model configuration to inject a payload in the binary and trigger command execution when the model starts.\n\n\n\nSnyk reported its findings to Menlo on February 18 and all issues were addressed by March 6. Four CVEs were issued: CVE-2025-2446 (arbitrary file write via path traversal), CVE-2025-2439 (out-of-bound read in GGUF parser), CVE-2025-2445 (command injection in Python engine model update), and CVE-2025-2447 (missing CSRF protection).\n\n\n\nRelated: New AI Security Tool Helps Organizations Set Trust Zones for Gen-AI Models\nRelated: New Jailbreak Technique Uses Fictional World to Manipulate AI\nRelated: New CCA Jailbreak Method Works Against Most AI Models\nRelated: Prompt Security Raises $18 Million for Gen-AI Security Platform\n\nThe AI was also found vulnerable to remote code execution (RCE), through Cortex.cpp’s support for python-engine. Because the engine is a C++ wrapper that executes the Python binary, an attacker can update the model configuration to inject a payload in the binary and trigger command execution when the model starts.\n\n\n\nSnyk reported its findings to Menlo on February 18 and all issues were addressed by March 6. Four CVEs were issued: CVE-2025-2446 (arbitrary file write via path traversal), CVE-2025-2439 (out-of-bound read in GGUF parser), CVE-2025-2445 (command injection in Python engine model update), and CVE-2025-2447 (missing CSRF protection).\n\n\n\nRelated: New AI Security Tool Helps Organizations Set Trust Zones for Gen-AI Models\nRelated: New Jailbreak Technique Uses Fictional World to Manipulate AI\nRelated: New CCA Jailbreak Method Works Against Most AI Models\nRelated: Prompt Security Raises $18 Million for Gen-AI Security Platform\n\nSnyk reported its findings to Menlo on February 18 and all issues were addressed by March 6. Four CVEs were issued: CVE-2025-2446 (arbitrary file write via path traversal), CVE-2025-2439 (out-of-bound read in GGUF parser), CVE-2025-2445 (command injection in Python engine model update), and CVE-2025-2447 (missing CSRF protection).\n\n\n\nRelated: New AI Security Tool Helps Organizations Set Trust Zones for Gen-AI Models\nRelated: New Jailbreak Technique Uses Fictional World to Manipulate AI\nRelated: New CCA Jailbreak Method Works Against Most AI Models\nRelated: Prompt Security Raises $18 Million for Gen-AI Security Platform\n\nRelated: New AI Security Tool Helps Organizations Set Trust Zones for Gen-AI Models\nRelated: New Jailbreak Technique Uses Fictional World to Manipulate AI\nRelated: New CCA Jailbreak Method Works Against Most AI Models\nRelated: Prompt Security Raises $18 Million for Gen-AI Security Platform\n\nRelated: New Jailbreak Technique Uses Fictional World to Manipulate AI\nRelated: New CCA Jailbreak Method Works Against Most AI Models\nRelated: Prompt Security Raises $18 Million for Gen-AI Security Platform\n\nRelated: New CCA Jailbreak Method Works Against Most AI Models\nRelated: Prompt Security Raises $18 Million for Gen-AI Security Platform\n\nRelated: Prompt Security Raises $18 Million for Gen-AI Security Platform\n\nIonut Arghire is an international correspondent for SecurityWeek.\nSubscribe to the SecurityWeek Email Briefing to stay informed on the latest threats, trends, and technology, along with insightful columns from industry experts.\nJoin this event as we dive into threat hunting tools and frameworks, and explore value of threat intelligence data in the defender’s security stack.\nLearn how integrating BAS and Automated Penetration Testing empowers security teams to quickly identify and validate threats, enabling prompt response and remediation.\nIdentity verification and fraud prevention firm Trulioo has appointed Vicky Bindra as CEO.\nCommvault has appointed Bill O'Connell as its Chief Security Officer.\nKevin Paige has been appointed Field CISO at identity governance company ConductorOne.\n\n\nThe rise of zero-knowledge threat actors powered by AI marks a turning point in the business of cybercrime where sophisticated attacks are no longer confined to skilled attackers.\n(Etay Maor)\n\n\n\nA strong security program will sometimes require substantial organizational and cultural changes around security practices, and inevitably, a higher cost.\n(Trevin Edgeworth)\n\n\n\nInterview with Taylor Pyle, a Cybersecurity Engineer at Viasat on her experience with both cyber and mentorship.\n(Marc Solomon)\n\n\n\nA Joni Mitchell song from the 1960s can teach us a lot about securing hybrid and multi-cloud environments.\n(Joshua Goldfarb)\n\n\n\nDefending high profile sporting events from adversarial attacks requires a mix of experienced capabilities and a solid threat intelligence program.\n(Marc Solomon)\n\nFlipboard\nReddit\nWhatsapp\nWhatsapp\nEmail\nGot a confidential news tip? We want to hear from you.\nReach a large audience of enterprise cybersecurity professionals \nSubscribe to the SecurityWeek Daily Briefing and get the latest content delivered to your inbox.\nCopyright © 2025 SecurityWeek ®, a Wired Business Media Publication. All Rights Reserved. \n\r\n\t\t\t\t\tSubscribe to the SecurityWeek Email Briefing to stay informed on the latest cybersecurity\r\n\t\t\t\t\tnews, threats, and expert insights. Unsubscribe at any time.\r\n\t\t\t\t\t\n\n\n"
    },
    {
        "Title": "Cyberhaven Banks $100 Million in Series D, Valuation Hits $1 Billion",
        "Link": "https://www.securityweek.com/cyberhaven-banks-100-million-in-series-d-valuation-hits-1-billion/",
        "Summary": "<p>Cyberhaven bags $100 million in funding at a billion-dollar valuation, a sign that investors remain bullish on data security startups.</p>\n<p>The post <a href=\"https://www.securityweek.com/cyberhaven-banks-100-million-in-series-d-valuation-hits-1-billion/\">Cyberhaven Banks $100 Million in Series D, Valuation Hits $1 Billion</a> appeared first on <a href=\"https://www.securityweek.com\">SecurityWeek</a>.</p>",
        "Date": "2025-04-02",
        "Content": "Hi, what are you looking for?\nCyberhaven bags $100 million in funding at a billion-dollar valuation, a sign that investors remain bullish on data security startups.\nBy\nFlipboard\nReddit\nWhatsapp\nWhatsapp\nEmail\nSilicon Valley data security startup Cyberhaven has bagged a hefty $100 million in new financing that values the company at $1 billion, a sign that investors remain bullish on companies building AI-powered enterprise security tooling.\nThe Palo Alto- based company said the Series D was provided by StepStone, Schroders and Industry Ventures and brings the total raised to $250 million.\n\n\n\nCyberhaven and its investors are betting big on a data detection and response platform designed to help organizations track and protect sensitive data. The company is pitching an AI-powered product that can help customers stop data exfiltration, understand data flows, train users on risky behavior in real time, and accelerate internal investigations. \n\n\n\nThe Cyberhaven portfolio also includes an autonomous AI agent designed to combat insider risks. \n\n\n\nThe company’s approach centers on “data lineage,” a method that maps the journey of data across an organization, providing real-time insights into the origins, transformation and flow of corporate data. \n\n\n\nLead investor StepStone likened the Cyberhaven technology to the creation of the EDR (Endpoint Detection and Response) category to replace outdated antivirus software. \n\n\n\n“Just as EDR revolutionized endpoint security by focusing on behavior rather than signatures, Cyberhaven’s Data Detection and Response approach is redefining data security by applying AI-based behavioral analysis to data,” said StepStone partner Seyonne Kang.\n\n\n\nCyberhaven said the plan is to use the new funding on product development, market expansion, and targeted acquisitions.Advertisement. Scroll to continue reading.\n\n\nRelated: Cyberhaven Chrome Extension Hack Linked to Widening Supply Chain Campaign\nRelated: Data Security Firm Cyberhaven Raises $88 Million at $488 Million Valuation\nRelated: Cyberhaven Raises $13 Million in Series A Funding\n\nThe Palo Alto- based company said the Series D was provided by StepStone, Schroders and Industry Ventures and brings the total raised to $250 million.\n\n\n\nCyberhaven and its investors are betting big on a data detection and response platform designed to help organizations track and protect sensitive data. The company is pitching an AI-powered product that can help customers stop data exfiltration, understand data flows, train users on risky behavior in real time, and accelerate internal investigations. \n\n\n\nThe Cyberhaven portfolio also includes an autonomous AI agent designed to combat insider risks. \n\n\n\nThe company’s approach centers on “data lineage,” a method that maps the journey of data across an organization, providing real-time insights into the origins, transformation and flow of corporate data. \n\n\n\nLead investor StepStone likened the Cyberhaven technology to the creation of the EDR (Endpoint Detection and Response) category to replace outdated antivirus software. \n\n\n\n“Just as EDR revolutionized endpoint security by focusing on behavior rather than signatures, Cyberhaven’s Data Detection and Response approach is redefining data security by applying AI-based behavioral analysis to data,” said StepStone partner Seyonne Kang.\n\n\n\nCyberhaven said the plan is to use the new funding on product development, market expansion, and targeted acquisitions.Advertisement. Scroll to continue reading.\n\n\nRelated: Cyberhaven Chrome Extension Hack Linked to Widening Supply Chain Campaign\nRelated: Data Security Firm Cyberhaven Raises $88 Million at $488 Million Valuation\nRelated: Cyberhaven Raises $13 Million in Series A Funding\n\nCyberhaven and its investors are betting big on a data detection and response platform designed to help organizations track and protect sensitive data. The company is pitching an AI-powered product that can help customers stop data exfiltration, understand data flows, train users on risky behavior in real time, and accelerate internal investigations. \n\n\n\nThe Cyberhaven portfolio also includes an autonomous AI agent designed to combat insider risks. \n\n\n\nThe company’s approach centers on “data lineage,” a method that maps the journey of data across an organization, providing real-time insights into the origins, transformation and flow of corporate data. \n\n\n\nLead investor StepStone likened the Cyberhaven technology to the creation of the EDR (Endpoint Detection and Response) category to replace outdated antivirus software. \n\n\n\n“Just as EDR revolutionized endpoint security by focusing on behavior rather than signatures, Cyberhaven’s Data Detection and Response approach is redefining data security by applying AI-based behavioral analysis to data,” said StepStone partner Seyonne Kang.\n\n\n\nCyberhaven said the plan is to use the new funding on product development, market expansion, and targeted acquisitions.Advertisement. Scroll to continue reading.\n\n\nRelated: Cyberhaven Chrome Extension Hack Linked to Widening Supply Chain Campaign\nRelated: Data Security Firm Cyberhaven Raises $88 Million at $488 Million Valuation\nRelated: Cyberhaven Raises $13 Million in Series A Funding\n\nThe Cyberhaven portfolio also includes an autonomous AI agent designed to combat insider risks. \n\n\n\nThe company’s approach centers on “data lineage,” a method that maps the journey of data across an organization, providing real-time insights into the origins, transformation and flow of corporate data. \n\n\n\nLead investor StepStone likened the Cyberhaven technology to the creation of the EDR (Endpoint Detection and Response) category to replace outdated antivirus software. \n\n\n\n“Just as EDR revolutionized endpoint security by focusing on behavior rather than signatures, Cyberhaven’s Data Detection and Response approach is redefining data security by applying AI-based behavioral analysis to data,” said StepStone partner Seyonne Kang.\n\n\n\nCyberhaven said the plan is to use the new funding on product development, market expansion, and targeted acquisitions.Advertisement. Scroll to continue reading.\n\n\nRelated: Cyberhaven Chrome Extension Hack Linked to Widening Supply Chain Campaign\nRelated: Data Security Firm Cyberhaven Raises $88 Million at $488 Million Valuation\nRelated: Cyberhaven Raises $13 Million in Series A Funding\n\nThe company’s approach centers on “data lineage,” a method that maps the journey of data across an organization, providing real-time insights into the origins, transformation and flow of corporate data. \n\n\n\nLead investor StepStone likened the Cyberhaven technology to the creation of the EDR (Endpoint Detection and Response) category to replace outdated antivirus software. \n\n\n\n“Just as EDR revolutionized endpoint security by focusing on behavior rather than signatures, Cyberhaven’s Data Detection and Response approach is redefining data security by applying AI-based behavioral analysis to data,” said StepStone partner Seyonne Kang.\n\n\n\nCyberhaven said the plan is to use the new funding on product development, market expansion, and targeted acquisitions.Advertisement. Scroll to continue reading.\n\n\nRelated: Cyberhaven Chrome Extension Hack Linked to Widening Supply Chain Campaign\nRelated: Data Security Firm Cyberhaven Raises $88 Million at $488 Million Valuation\nRelated: Cyberhaven Raises $13 Million in Series A Funding\n\nLead investor StepStone likened the Cyberhaven technology to the creation of the EDR (Endpoint Detection and Response) category to replace outdated antivirus software. \n\n\n\n“Just as EDR revolutionized endpoint security by focusing on behavior rather than signatures, Cyberhaven’s Data Detection and Response approach is redefining data security by applying AI-based behavioral analysis to data,” said StepStone partner Seyonne Kang.\n\n\n\nCyberhaven said the plan is to use the new funding on product development, market expansion, and targeted acquisitions.Advertisement. Scroll to continue reading.\n\n\nRelated: Cyberhaven Chrome Extension Hack Linked to Widening Supply Chain Campaign\nRelated: Data Security Firm Cyberhaven Raises $88 Million at $488 Million Valuation\nRelated: Cyberhaven Raises $13 Million in Series A Funding\n\n“Just as EDR revolutionized endpoint security by focusing on behavior rather than signatures, Cyberhaven’s Data Detection and Response approach is redefining data security by applying AI-based behavioral analysis to data,” said StepStone partner Seyonne Kang.\n\n\n\nCyberhaven said the plan is to use the new funding on product development, market expansion, and targeted acquisitions.Advertisement. Scroll to continue reading.\n\n\nRelated: Cyberhaven Chrome Extension Hack Linked to Widening Supply Chain Campaign\nRelated: Data Security Firm Cyberhaven Raises $88 Million at $488 Million Valuation\nRelated: Cyberhaven Raises $13 Million in Series A Funding\n\nCyberhaven said the plan is to use the new funding on product development, market expansion, and targeted acquisitions.Advertisement. Scroll to continue reading.\n\n\nRelated: Cyberhaven Chrome Extension Hack Linked to Widening Supply Chain Campaign\nRelated: Data Security Firm Cyberhaven Raises $88 Million at $488 Million Valuation\nRelated: Cyberhaven Raises $13 Million in Series A Funding\n\nRelated: Cyberhaven Chrome Extension Hack Linked to Widening Supply Chain Campaign\nRelated: Data Security Firm Cyberhaven Raises $88 Million at $488 Million Valuation\nRelated: Cyberhaven Raises $13 Million in Series A Funding\n\nRelated: Data Security Firm Cyberhaven Raises $88 Million at $488 Million Valuation\nRelated: Cyberhaven Raises $13 Million in Series A Funding\n\nRelated: Cyberhaven Raises $13 Million in Series A Funding\n\nRyan Naraine is Editor-at-Large at SecurityWeek and host of the popular Security Conversations podcast series. He is a security community engagement expert who has built programs at major global brands, including Intel Corp., Bishop Fox and GReAT.  Ryan is a founding-director of the Security Tinkerers non-profit, an advisor to early-stage entrepreneurs, and a regular speaker at security conferences around the world.\nSubscribe to the SecurityWeek Email Briefing to stay informed on the latest threats, trends, and technology, along with insightful columns from industry experts.\nJoin this event as we dive into threat hunting tools and frameworks, and explore value of threat intelligence data in the defender’s security stack.\nLearn how integrating BAS and Automated Penetration Testing empowers security teams to quickly identify and validate threats, enabling prompt response and remediation.\nIdentity verification and fraud prevention firm Trulioo has appointed Vicky Bindra as CEO.\nCommvault has appointed Bill O'Connell as its Chief Security Officer.\nKevin Paige has been appointed Field CISO at identity governance company ConductorOne.\n\n\nThe rise of zero-knowledge threat actors powered by AI marks a turning point in the business of cybercrime where sophisticated attacks are no longer confined to skilled attackers.\n(Etay Maor)\n\n\n\nA strong security program will sometimes require substantial organizational and cultural changes around security practices, and inevitably, a higher cost.\n(Trevin Edgeworth)\n\n\n\nInterview with Taylor Pyle, a Cybersecurity Engineer at Viasat on her experience with both cyber and mentorship.\n(Marc Solomon)\n\n\n\nA Joni Mitchell song from the 1960s can teach us a lot about securing hybrid and multi-cloud environments.\n(Joshua Goldfarb)\n\n\n\nDefending high profile sporting events from adversarial attacks requires a mix of experienced capabilities and a solid threat intelligence program.\n(Marc Solomon)\n\nFlipboard\nReddit\nWhatsapp\nWhatsapp\nEmail\nGot a confidential news tip? We want to hear from you.\nReach a large audience of enterprise cybersecurity professionals \nSubscribe to the SecurityWeek Daily Briefing and get the latest content delivered to your inbox.\nCopyright © 2025 SecurityWeek ®, a Wired Business Media Publication. All Rights Reserved. \n\r\n\t\t\t\t\tSubscribe to the SecurityWeek Email Briefing to stay informed on the latest cybersecurity\r\n\t\t\t\t\tnews, threats, and expert insights. Unsubscribe at any time.\r\n\t\t\t\t\t\n\n\n"
    },
    {
        "Title": "AI Giving Rise of the ‘Zero-Knowledge’ Threat Actor",
        "Link": "https://www.securityweek.com/ai-giving-rise-of-the-zero-knowledge-threat-actor/",
        "Summary": "<p>The rise of zero-knowledge threat actors powered by AI marks a turning point in the business of cybercrime where sophisticated attacks are no longer confined to skilled attackers.</p>\n<p>The post <a href=\"https://www.securityweek.com/ai-giving-rise-of-the-zero-knowledge-threat-actor/\">AI Giving Rise of the ‘Zero-Knowledge’ Threat Actor</a> appeared first on <a href=\"https://www.securityweek.com\">SecurityWeek</a>.</p>",
        "Date": "2025-04-02",
        "Content": "Hi, what are you looking for?\nThe rise of zero-knowledge threat actors powered by AI marks a turning point in the business of cybercrime where sophisticated attacks are no longer confined to skilled attackers.\nBy\nFlipboard\nReddit\nWhatsapp\nWhatsapp\nEmail\nArtificial intelligence is a double-edged sword. On one side, AI empowers people to do their jobs better and faster while on the other, it enables people with malicious intent to become scammers, hacktivists and cyber criminals.\nRise of the Zero-Knowledge Threat Actor\nThe business of cybercrime is highly lucrative; however, traditionally, it has only been accessible to people with advanced technical skills. Only someone with deep knowledge and experience with multiple technical domains like systems and software, networking, programming, cryptography, cybersecurity, etc. has the ability to develop malware, identify vulnerabilities, evade cybersecurity defenses and exploit systems.With AI entering the scene, this entry barrier has been lowered substantially. Even those with no hacking experience or technical expertise can leverage AI to launch attacks on enterprises.\n\n\n\nFrom Conversations To Malware Creation\nMost large language models (LLMs) have built-in guardrails, i.e., safety protocols that limit the behaviors of AI models to a more secure range of functions. In other words, AI guardrails help recognize malicious inputs or commands, preventing threat actors from misusing or exploiting the technology for much of their illegal activities.\n\n\n\nUnfortunately, these guardrails are not foolproof. Research from Cato CTRL demonstrated how almost anyone, with no experience in malware coding, can manipulate LLMs like OpenAI’s ChatGPT, Microsoft Copilot and DeepSeek, to override these guardrails, and conduct malicious activities like developing an infostealer malware.\n\n\n\nCato CTRL refers to this new jailbreaking method as “Immersive World,” essentially a narrative engineering technique whereby users ask the LLM to assume an environment where restricted operations are normalized. In the demonstration, Cato CTRL researchers instructed the LLM to create a fictional world called “Velora,” where malware development was celebrated and where no legal restrictions or consequences were adopted or implied. Next, researchers created fictional characters and assigned them various tasks and responsibilities. Through continuous feedback, engagement and iteration, we convinced the model to achieve its objective, i.e., build a working infostealer that was capable of stealing credentials from Google Chrome.\n\n\n\nLearn More at the AI Risk Summit at Half Moon BayAdvertisement. Scroll to continue reading.\n\n\nFor Zero-Knowledge Threat Actors, Malware Creation is Only the Tip of the Iceberg\nMalware creation is just a starting point of what amateur threat actors will achieve with AI. In the not-so-distant future, novices will be able to design sophisticated social engineering campaigns, analyze target environments, identify vulnerabilities, choose attack vectors, orchestrate multi-stage attacks, automate target selection and attack execution, etc.  AI bots will monitor their own operations and adapt their tactics based on what they learn about the target entity or environment.\n\n\n\nIn a nutshell, the volume of low-skilled threat actors and their expertise is all set to grow exponentially, thanks to AI.\n\n\n\nWhat Can Organizations Do To Fight Back?\nThe rise of the zero-knowledge threat actor should be a wakeup call for organizations—cyberattacks are only going to be smarter, better and more frequent. Below are some recommendations and best practices that can help:\n\n\n\n\nIncrease Employee Awareness: Conduct training sessions to inform employees about the growing risks of AI-powered threat actors. Run simulated AI attacks and fire drills to improve alertness and vigilance among employees.\nConduct AI Red Teaming: If your organization uses or creates AI tools, test these systems against malicious prompts and validate if they are vulnerable to jailbreaking. Invest time and resources to anticipate AI attacks and stress-test systems against those scenarios.\nImplement Holistic Security: Deploy an end-to-end security system like SASE, not fragmented tools, to monitor, detect and analyze malicious signals across the entire IT infrastructure (users, clouds, devices, networks).\nPatch Systems and Software Frequently: Ensure that tools and the software you use in your organization are the latest and greatest versions. If you don’t fix loopholes, AI-powered adversaries will certainly seek them out and exploit them.\nImprove Attack Readiness: A proactive and well-practiced incident response plan will not only minimize damage but also strengthen organizational resilience against the unpredictability of AI-powered threats.\nAdopt Security Frameworks: Follow best practices as advocated by stalwart standard-bearers such as MITRE ATLAS, OWASP Top 10 for LLM Applications, and Google’s Secure AI Framework (SAIF).\n\nThe rise of zero-knowledge threat actors powered by AI marks a turning point in the business of cybercrime where sophisticated attacks are no longer confined to skilled attackers. By conducting red teaming exercises, implementing a holistic security system that offers in-depth visibility and total control over attack surfaces, and improving attack readiness, organizations can ensure preparedness for both current and future challenges in this new era of AI-powered cybercrime.\n\t\t\t\nRise of the Zero-Knowledge Threat Actor\nThe business of cybercrime is highly lucrative; however, traditionally, it has only been accessible to people with advanced technical skills. Only someone with deep knowledge and experience with multiple technical domains like systems and software, networking, programming, cryptography, cybersecurity, etc. has the ability to develop malware, identify vulnerabilities, evade cybersecurity defenses and exploit systems.With AI entering the scene, this entry barrier has been lowered substantially. Even those with no hacking experience or technical expertise can leverage AI to launch attacks on enterprises.\n\n\n\nFrom Conversations To Malware Creation\nMost large language models (LLMs) have built-in guardrails, i.e., safety protocols that limit the behaviors of AI models to a more secure range of functions. In other words, AI guardrails help recognize malicious inputs or commands, preventing threat actors from misusing or exploiting the technology for much of their illegal activities.\n\n\n\nUnfortunately, these guardrails are not foolproof. Research from Cato CTRL demonstrated how almost anyone, with no experience in malware coding, can manipulate LLMs like OpenAI’s ChatGPT, Microsoft Copilot and DeepSeek, to override these guardrails, and conduct malicious activities like developing an infostealer malware.\n\n\n\nCato CTRL refers to this new jailbreaking method as “Immersive World,” essentially a narrative engineering technique whereby users ask the LLM to assume an environment where restricted operations are normalized. In the demonstration, Cato CTRL researchers instructed the LLM to create a fictional world called “Velora,” where malware development was celebrated and where no legal restrictions or consequences were adopted or implied. Next, researchers created fictional characters and assigned them various tasks and responsibilities. Through continuous feedback, engagement and iteration, we convinced the model to achieve its objective, i.e., build a working infostealer that was capable of stealing credentials from Google Chrome.\n\n\n\nLearn More at the AI Risk Summit at Half Moon BayAdvertisement. Scroll to continue reading.\n\n\nFor Zero-Knowledge Threat Actors, Malware Creation is Only the Tip of the Iceberg\nMalware creation is just a starting point of what amateur threat actors will achieve with AI. In the not-so-distant future, novices will be able to design sophisticated social engineering campaigns, analyze target environments, identify vulnerabilities, choose attack vectors, orchestrate multi-stage attacks, automate target selection and attack execution, etc.  AI bots will monitor their own operations and adapt their tactics based on what they learn about the target entity or environment.\n\n\n\nIn a nutshell, the volume of low-skilled threat actors and their expertise is all set to grow exponentially, thanks to AI.\n\n\n\nWhat Can Organizations Do To Fight Back?\nThe rise of the zero-knowledge threat actor should be a wakeup call for organizations—cyberattacks are only going to be smarter, better and more frequent. Below are some recommendations and best practices that can help:\n\n\n\n\nIncrease Employee Awareness: Conduct training sessions to inform employees about the growing risks of AI-powered threat actors. Run simulated AI attacks and fire drills to improve alertness and vigilance among employees.\nConduct AI Red Teaming: If your organization uses or creates AI tools, test these systems against malicious prompts and validate if they are vulnerable to jailbreaking. Invest time and resources to anticipate AI attacks and stress-test systems against those scenarios.\nImplement Holistic Security: Deploy an end-to-end security system like SASE, not fragmented tools, to monitor, detect and analyze malicious signals across the entire IT infrastructure (users, clouds, devices, networks).\nPatch Systems and Software Frequently: Ensure that tools and the software you use in your organization are the latest and greatest versions. If you don’t fix loopholes, AI-powered adversaries will certainly seek them out and exploit them.\nImprove Attack Readiness: A proactive and well-practiced incident response plan will not only minimize damage but also strengthen organizational resilience against the unpredictability of AI-powered threats.\nAdopt Security Frameworks: Follow best practices as advocated by stalwart standard-bearers such as MITRE ATLAS, OWASP Top 10 for LLM Applications, and Google’s Secure AI Framework (SAIF).\n\nThe rise of zero-knowledge threat actors powered by AI marks a turning point in the business of cybercrime where sophisticated attacks are no longer confined to skilled attackers. By conducting red teaming exercises, implementing a holistic security system that offers in-depth visibility and total control over attack surfaces, and improving attack readiness, organizations can ensure preparedness for both current and future challenges in this new era of AI-powered cybercrime.\n\t\t\t\nThe business of cybercrime is highly lucrative; however, traditionally, it has only been accessible to people with advanced technical skills. Only someone with deep knowledge and experience with multiple technical domains like systems and software, networking, programming, cryptography, cybersecurity, etc. has the ability to develop malware, identify vulnerabilities, evade cybersecurity defenses and exploit systems.With AI entering the scene, this entry barrier has been lowered substantially. Even those with no hacking experience or technical expertise can leverage AI to launch attacks on enterprises.\n\n\n\nFrom Conversations To Malware Creation\nMost large language models (LLMs) have built-in guardrails, i.e., safety protocols that limit the behaviors of AI models to a more secure range of functions. In other words, AI guardrails help recognize malicious inputs or commands, preventing threat actors from misusing or exploiting the technology for much of their illegal activities.\n\n\n\nUnfortunately, these guardrails are not foolproof. Research from Cato CTRL demonstrated how almost anyone, with no experience in malware coding, can manipulate LLMs like OpenAI’s ChatGPT, Microsoft Copilot and DeepSeek, to override these guardrails, and conduct malicious activities like developing an infostealer malware.\n\n\n\nCato CTRL refers to this new jailbreaking method as “Immersive World,” essentially a narrative engineering technique whereby users ask the LLM to assume an environment where restricted operations are normalized. In the demonstration, Cato CTRL researchers instructed the LLM to create a fictional world called “Velora,” where malware development was celebrated and where no legal restrictions or consequences were adopted or implied. Next, researchers created fictional characters and assigned them various tasks and responsibilities. Through continuous feedback, engagement and iteration, we convinced the model to achieve its objective, i.e., build a working infostealer that was capable of stealing credentials from Google Chrome.\n\n\n\nLearn More at the AI Risk Summit at Half Moon BayAdvertisement. Scroll to continue reading.\n\n\nFor Zero-Knowledge Threat Actors, Malware Creation is Only the Tip of the Iceberg\nMalware creation is just a starting point of what amateur threat actors will achieve with AI. In the not-so-distant future, novices will be able to design sophisticated social engineering campaigns, analyze target environments, identify vulnerabilities, choose attack vectors, orchestrate multi-stage attacks, automate target selection and attack execution, etc.  AI bots will monitor their own operations and adapt their tactics based on what they learn about the target entity or environment.\n\n\n\nIn a nutshell, the volume of low-skilled threat actors and their expertise is all set to grow exponentially, thanks to AI.\n\n\n\nWhat Can Organizations Do To Fight Back?\nThe rise of the zero-knowledge threat actor should be a wakeup call for organizations—cyberattacks are only going to be smarter, better and more frequent. Below are some recommendations and best practices that can help:\n\n\n\n\nIncrease Employee Awareness: Conduct training sessions to inform employees about the growing risks of AI-powered threat actors. Run simulated AI attacks and fire drills to improve alertness and vigilance among employees.\nConduct AI Red Teaming: If your organization uses or creates AI tools, test these systems against malicious prompts and validate if they are vulnerable to jailbreaking. Invest time and resources to anticipate AI attacks and stress-test systems against those scenarios.\nImplement Holistic Security: Deploy an end-to-end security system like SASE, not fragmented tools, to monitor, detect and analyze malicious signals across the entire IT infrastructure (users, clouds, devices, networks).\nPatch Systems and Software Frequently: Ensure that tools and the software you use in your organization are the latest and greatest versions. If you don’t fix loopholes, AI-powered adversaries will certainly seek them out and exploit them.\nImprove Attack Readiness: A proactive and well-practiced incident response plan will not only minimize damage but also strengthen organizational resilience against the unpredictability of AI-powered threats.\nAdopt Security Frameworks: Follow best practices as advocated by stalwart standard-bearers such as MITRE ATLAS, OWASP Top 10 for LLM Applications, and Google’s Secure AI Framework (SAIF).\n\nThe rise of zero-knowledge threat actors powered by AI marks a turning point in the business of cybercrime where sophisticated attacks are no longer confined to skilled attackers. By conducting red teaming exercises, implementing a holistic security system that offers in-depth visibility and total control over attack surfaces, and improving attack readiness, organizations can ensure preparedness for both current and future challenges in this new era of AI-powered cybercrime.\n\t\t\t\nFrom Conversations To Malware Creation\nMost large language models (LLMs) have built-in guardrails, i.e., safety protocols that limit the behaviors of AI models to a more secure range of functions. In other words, AI guardrails help recognize malicious inputs or commands, preventing threat actors from misusing or exploiting the technology for much of their illegal activities.\n\n\n\nUnfortunately, these guardrails are not foolproof. Research from Cato CTRL demonstrated how almost anyone, with no experience in malware coding, can manipulate LLMs like OpenAI’s ChatGPT, Microsoft Copilot and DeepSeek, to override these guardrails, and conduct malicious activities like developing an infostealer malware.\n\n\n\nCato CTRL refers to this new jailbreaking method as “Immersive World,” essentially a narrative engineering technique whereby users ask the LLM to assume an environment where restricted operations are normalized. In the demonstration, Cato CTRL researchers instructed the LLM to create a fictional world called “Velora,” where malware development was celebrated and where no legal restrictions or consequences were adopted or implied. Next, researchers created fictional characters and assigned them various tasks and responsibilities. Through continuous feedback, engagement and iteration, we convinced the model to achieve its objective, i.e., build a working infostealer that was capable of stealing credentials from Google Chrome.\n\n\n\nLearn More at the AI Risk Summit at Half Moon BayAdvertisement. Scroll to continue reading.\n\n\nFor Zero-Knowledge Threat Actors, Malware Creation is Only the Tip of the Iceberg\nMalware creation is just a starting point of what amateur threat actors will achieve with AI. In the not-so-distant future, novices will be able to design sophisticated social engineering campaigns, analyze target environments, identify vulnerabilities, choose attack vectors, orchestrate multi-stage attacks, automate target selection and attack execution, etc.  AI bots will monitor their own operations and adapt their tactics based on what they learn about the target entity or environment.\n\n\n\nIn a nutshell, the volume of low-skilled threat actors and their expertise is all set to grow exponentially, thanks to AI.\n\n\n\nWhat Can Organizations Do To Fight Back?\nThe rise of the zero-knowledge threat actor should be a wakeup call for organizations—cyberattacks are only going to be smarter, better and more frequent. Below are some recommendations and best practices that can help:\n\n\n\n\nIncrease Employee Awareness: Conduct training sessions to inform employees about the growing risks of AI-powered threat actors. Run simulated AI attacks and fire drills to improve alertness and vigilance among employees.\nConduct AI Red Teaming: If your organization uses or creates AI tools, test these systems against malicious prompts and validate if they are vulnerable to jailbreaking. Invest time and resources to anticipate AI attacks and stress-test systems against those scenarios.\nImplement Holistic Security: Deploy an end-to-end security system like SASE, not fragmented tools, to monitor, detect and analyze malicious signals across the entire IT infrastructure (users, clouds, devices, networks).\nPatch Systems and Software Frequently: Ensure that tools and the software you use in your organization are the latest and greatest versions. If you don’t fix loopholes, AI-powered adversaries will certainly seek them out and exploit them.\nImprove Attack Readiness: A proactive and well-practiced incident response plan will not only minimize damage but also strengthen organizational resilience against the unpredictability of AI-powered threats.\nAdopt Security Frameworks: Follow best practices as advocated by stalwart standard-bearers such as MITRE ATLAS, OWASP Top 10 for LLM Applications, and Google’s Secure AI Framework (SAIF).\n\nThe rise of zero-knowledge threat actors powered by AI marks a turning point in the business of cybercrime where sophisticated attacks are no longer confined to skilled attackers. By conducting red teaming exercises, implementing a holistic security system that offers in-depth visibility and total control over attack surfaces, and improving attack readiness, organizations can ensure preparedness for both current and future challenges in this new era of AI-powered cybercrime.\n\t\t\t\nMost large language models (LLMs) have built-in guardrails, i.e., safety protocols that limit the behaviors of AI models to a more secure range of functions. In other words, AI guardrails help recognize malicious inputs or commands, preventing threat actors from misusing or exploiting the technology for much of their illegal activities.\n\n\n\nUnfortunately, these guardrails are not foolproof. Research from Cato CTRL demonstrated how almost anyone, with no experience in malware coding, can manipulate LLMs like OpenAI’s ChatGPT, Microsoft Copilot and DeepSeek, to override these guardrails, and conduct malicious activities like developing an infostealer malware.\n\n\n\nCato CTRL refers to this new jailbreaking method as “Immersive World,” essentially a narrative engineering technique whereby users ask the LLM to assume an environment where restricted operations are normalized. In the demonstration, Cato CTRL researchers instructed the LLM to create a fictional world called “Velora,” where malware development was celebrated and where no legal restrictions or consequences were adopted or implied. Next, researchers created fictional characters and assigned them various tasks and responsibilities. Through continuous feedback, engagement and iteration, we convinced the model to achieve its objective, i.e., build a working infostealer that was capable of stealing credentials from Google Chrome.\n\n\n\nLearn More at the AI Risk Summit at Half Moon BayAdvertisement. Scroll to continue reading.\n\n\nFor Zero-Knowledge Threat Actors, Malware Creation is Only the Tip of the Iceberg\nMalware creation is just a starting point of what amateur threat actors will achieve with AI. In the not-so-distant future, novices will be able to design sophisticated social engineering campaigns, analyze target environments, identify vulnerabilities, choose attack vectors, orchestrate multi-stage attacks, automate target selection and attack execution, etc.  AI bots will monitor their own operations and adapt their tactics based on what they learn about the target entity or environment.\n\n\n\nIn a nutshell, the volume of low-skilled threat actors and their expertise is all set to grow exponentially, thanks to AI.\n\n\n\nWhat Can Organizations Do To Fight Back?\nThe rise of the zero-knowledge threat actor should be a wakeup call for organizations—cyberattacks are only going to be smarter, better and more frequent. Below are some recommendations and best practices that can help:\n\n\n\n\nIncrease Employee Awareness: Conduct training sessions to inform employees about the growing risks of AI-powered threat actors. Run simulated AI attacks and fire drills to improve alertness and vigilance among employees.\nConduct AI Red Teaming: If your organization uses or creates AI tools, test these systems against malicious prompts and validate if they are vulnerable to jailbreaking. Invest time and resources to anticipate AI attacks and stress-test systems against those scenarios.\nImplement Holistic Security: Deploy an end-to-end security system like SASE, not fragmented tools, to monitor, detect and analyze malicious signals across the entire IT infrastructure (users, clouds, devices, networks).\nPatch Systems and Software Frequently: Ensure that tools and the software you use in your organization are the latest and greatest versions. If you don’t fix loopholes, AI-powered adversaries will certainly seek them out and exploit them.\nImprove Attack Readiness: A proactive and well-practiced incident response plan will not only minimize damage but also strengthen organizational resilience against the unpredictability of AI-powered threats.\nAdopt Security Frameworks: Follow best practices as advocated by stalwart standard-bearers such as MITRE ATLAS, OWASP Top 10 for LLM Applications, and Google’s Secure AI Framework (SAIF).\n\nThe rise of zero-knowledge threat actors powered by AI marks a turning point in the business of cybercrime where sophisticated attacks are no longer confined to skilled attackers. By conducting red teaming exercises, implementing a holistic security system that offers in-depth visibility and total control over attack surfaces, and improving attack readiness, organizations can ensure preparedness for both current and future challenges in this new era of AI-powered cybercrime.\n\t\t\t\nUnfortunately, these guardrails are not foolproof. Research from Cato CTRL demonstrated how almost anyone, with no experience in malware coding, can manipulate LLMs like OpenAI’s ChatGPT, Microsoft Copilot and DeepSeek, to override these guardrails, and conduct malicious activities like developing an infostealer malware.\n\n\n\nCato CTRL refers to this new jailbreaking method as “Immersive World,” essentially a narrative engineering technique whereby users ask the LLM to assume an environment where restricted operations are normalized. In the demonstration, Cato CTRL researchers instructed the LLM to create a fictional world called “Velora,” where malware development was celebrated and where no legal restrictions or consequences were adopted or implied. Next, researchers created fictional characters and assigned them various tasks and responsibilities. Through continuous feedback, engagement and iteration, we convinced the model to achieve its objective, i.e., build a working infostealer that was capable of stealing credentials from Google Chrome.\n\n\n\nLearn More at the AI Risk Summit at Half Moon BayAdvertisement. Scroll to continue reading.\n\n\nFor Zero-Knowledge Threat Actors, Malware Creation is Only the Tip of the Iceberg\nMalware creation is just a starting point of what amateur threat actors will achieve with AI. In the not-so-distant future, novices will be able to design sophisticated social engineering campaigns, analyze target environments, identify vulnerabilities, choose attack vectors, orchestrate multi-stage attacks, automate target selection and attack execution, etc.  AI bots will monitor their own operations and adapt their tactics based on what they learn about the target entity or environment.\n\n\n\nIn a nutshell, the volume of low-skilled threat actors and their expertise is all set to grow exponentially, thanks to AI.\n\n\n\nWhat Can Organizations Do To Fight Back?\nThe rise of the zero-knowledge threat actor should be a wakeup call for organizations—cyberattacks are only going to be smarter, better and more frequent. Below are some recommendations and best practices that can help:\n\n\n\n\nIncrease Employee Awareness: Conduct training sessions to inform employees about the growing risks of AI-powered threat actors. Run simulated AI attacks and fire drills to improve alertness and vigilance among employees.\nConduct AI Red Teaming: If your organization uses or creates AI tools, test these systems against malicious prompts and validate if they are vulnerable to jailbreaking. Invest time and resources to anticipate AI attacks and stress-test systems against those scenarios.\nImplement Holistic Security: Deploy an end-to-end security system like SASE, not fragmented tools, to monitor, detect and analyze malicious signals across the entire IT infrastructure (users, clouds, devices, networks).\nPatch Systems and Software Frequently: Ensure that tools and the software you use in your organization are the latest and greatest versions. If you don’t fix loopholes, AI-powered adversaries will certainly seek them out and exploit them.\nImprove Attack Readiness: A proactive and well-practiced incident response plan will not only minimize damage but also strengthen organizational resilience against the unpredictability of AI-powered threats.\nAdopt Security Frameworks: Follow best practices as advocated by stalwart standard-bearers such as MITRE ATLAS, OWASP Top 10 for LLM Applications, and Google’s Secure AI Framework (SAIF).\n\nThe rise of zero-knowledge threat actors powered by AI marks a turning point in the business of cybercrime where sophisticated attacks are no longer confined to skilled attackers. By conducting red teaming exercises, implementing a holistic security system that offers in-depth visibility and total control over attack surfaces, and improving attack readiness, organizations can ensure preparedness for both current and future challenges in this new era of AI-powered cybercrime.\n\t\t\t\nCato CTRL refers to this new jailbreaking method as “Immersive World,” essentially a narrative engineering technique whereby users ask the LLM to assume an environment where restricted operations are normalized. In the demonstration, Cato CTRL researchers instructed the LLM to create a fictional world called “Velora,” where malware development was celebrated and where no legal restrictions or consequences were adopted or implied. Next, researchers created fictional characters and assigned them various tasks and responsibilities. Through continuous feedback, engagement and iteration, we convinced the model to achieve its objective, i.e., build a working infostealer that was capable of stealing credentials from Google Chrome.\n\n\n\nLearn More at the AI Risk Summit at Half Moon BayAdvertisement. Scroll to continue reading.\n\n\nFor Zero-Knowledge Threat Actors, Malware Creation is Only the Tip of the Iceberg\nMalware creation is just a starting point of what amateur threat actors will achieve with AI. In the not-so-distant future, novices will be able to design sophisticated social engineering campaigns, analyze target environments, identify vulnerabilities, choose attack vectors, orchestrate multi-stage attacks, automate target selection and attack execution, etc.  AI bots will monitor their own operations and adapt their tactics based on what they learn about the target entity or environment.\n\n\n\nIn a nutshell, the volume of low-skilled threat actors and their expertise is all set to grow exponentially, thanks to AI.\n\n\n\nWhat Can Organizations Do To Fight Back?\nThe rise of the zero-knowledge threat actor should be a wakeup call for organizations—cyberattacks are only going to be smarter, better and more frequent. Below are some recommendations and best practices that can help:\n\n\n\n\nIncrease Employee Awareness: Conduct training sessions to inform employees about the growing risks of AI-powered threat actors. Run simulated AI attacks and fire drills to improve alertness and vigilance among employees.\nConduct AI Red Teaming: If your organization uses or creates AI tools, test these systems against malicious prompts and validate if they are vulnerable to jailbreaking. Invest time and resources to anticipate AI attacks and stress-test systems against those scenarios.\nImplement Holistic Security: Deploy an end-to-end security system like SASE, not fragmented tools, to monitor, detect and analyze malicious signals across the entire IT infrastructure (users, clouds, devices, networks).\nPatch Systems and Software Frequently: Ensure that tools and the software you use in your organization are the latest and greatest versions. If you don’t fix loopholes, AI-powered adversaries will certainly seek them out and exploit them.\nImprove Attack Readiness: A proactive and well-practiced incident response plan will not only minimize damage but also strengthen organizational resilience against the unpredictability of AI-powered threats.\nAdopt Security Frameworks: Follow best practices as advocated by stalwart standard-bearers such as MITRE ATLAS, OWASP Top 10 for LLM Applications, and Google’s Secure AI Framework (SAIF).\n\nThe rise of zero-knowledge threat actors powered by AI marks a turning point in the business of cybercrime where sophisticated attacks are no longer confined to skilled attackers. By conducting red teaming exercises, implementing a holistic security system that offers in-depth visibility and total control over attack surfaces, and improving attack readiness, organizations can ensure preparedness for both current and future challenges in this new era of AI-powered cybercrime.\n\t\t\t\nLearn More at the AI Risk Summit at Half Moon BayAdvertisement. Scroll to continue reading.\n\n\nFor Zero-Knowledge Threat Actors, Malware Creation is Only the Tip of the Iceberg\nMalware creation is just a starting point of what amateur threat actors will achieve with AI. In the not-so-distant future, novices will be able to design sophisticated social engineering campaigns, analyze target environments, identify vulnerabilities, choose attack vectors, orchestrate multi-stage attacks, automate target selection and attack execution, etc.  AI bots will monitor their own operations and adapt their tactics based on what they learn about the target entity or environment.\n\n\n\nIn a nutshell, the volume of low-skilled threat actors and their expertise is all set to grow exponentially, thanks to AI.\n\n\n\nWhat Can Organizations Do To Fight Back?\nThe rise of the zero-knowledge threat actor should be a wakeup call for organizations—cyberattacks are only going to be smarter, better and more frequent. Below are some recommendations and best practices that can help:\n\n\n\n\nIncrease Employee Awareness: Conduct training sessions to inform employees about the growing risks of AI-powered threat actors. Run simulated AI attacks and fire drills to improve alertness and vigilance among employees.\nConduct AI Red Teaming: If your organization uses or creates AI tools, test these systems against malicious prompts and validate if they are vulnerable to jailbreaking. Invest time and resources to anticipate AI attacks and stress-test systems against those scenarios.\nImplement Holistic Security: Deploy an end-to-end security system like SASE, not fragmented tools, to monitor, detect and analyze malicious signals across the entire IT infrastructure (users, clouds, devices, networks).\nPatch Systems and Software Frequently: Ensure that tools and the software you use in your organization are the latest and greatest versions. If you don’t fix loopholes, AI-powered adversaries will certainly seek them out and exploit them.\nImprove Attack Readiness: A proactive and well-practiced incident response plan will not only minimize damage but also strengthen organizational resilience against the unpredictability of AI-powered threats.\nAdopt Security Frameworks: Follow best practices as advocated by stalwart standard-bearers such as MITRE ATLAS, OWASP Top 10 for LLM Applications, and Google’s Secure AI Framework (SAIF).\n\nThe rise of zero-knowledge threat actors powered by AI marks a turning point in the business of cybercrime where sophisticated attacks are no longer confined to skilled attackers. By conducting red teaming exercises, implementing a holistic security system that offers in-depth visibility and total control over attack surfaces, and improving attack readiness, organizations can ensure preparedness for both current and future challenges in this new era of AI-powered cybercrime.\n\t\t\t\nFor Zero-Knowledge Threat Actors, Malware Creation is Only the Tip of the Iceberg\nMalware creation is just a starting point of what amateur threat actors will achieve with AI. In the not-so-distant future, novices will be able to design sophisticated social engineering campaigns, analyze target environments, identify vulnerabilities, choose attack vectors, orchestrate multi-stage attacks, automate target selection and attack execution, etc.  AI bots will monitor their own operations and adapt their tactics based on what they learn about the target entity or environment.\n\n\n\nIn a nutshell, the volume of low-skilled threat actors and their expertise is all set to grow exponentially, thanks to AI.\n\n\n\nWhat Can Organizations Do To Fight Back?\nThe rise of the zero-knowledge threat actor should be a wakeup call for organizations—cyberattacks are only going to be smarter, better and more frequent. Below are some recommendations and best practices that can help:\n\n\n\n\nIncrease Employee Awareness: Conduct training sessions to inform employees about the growing risks of AI-powered threat actors. Run simulated AI attacks and fire drills to improve alertness and vigilance among employees.\nConduct AI Red Teaming: If your organization uses or creates AI tools, test these systems against malicious prompts and validate if they are vulnerable to jailbreaking. Invest time and resources to anticipate AI attacks and stress-test systems against those scenarios.\nImplement Holistic Security: Deploy an end-to-end security system like SASE, not fragmented tools, to monitor, detect and analyze malicious signals across the entire IT infrastructure (users, clouds, devices, networks).\nPatch Systems and Software Frequently: Ensure that tools and the software you use in your organization are the latest and greatest versions. If you don’t fix loopholes, AI-powered adversaries will certainly seek them out and exploit them.\nImprove Attack Readiness: A proactive and well-practiced incident response plan will not only minimize damage but also strengthen organizational resilience against the unpredictability of AI-powered threats.\nAdopt Security Frameworks: Follow best practices as advocated by stalwart standard-bearers such as MITRE ATLAS, OWASP Top 10 for LLM Applications, and Google’s Secure AI Framework (SAIF).\n\nThe rise of zero-knowledge threat actors powered by AI marks a turning point in the business of cybercrime where sophisticated attacks are no longer confined to skilled attackers. By conducting red teaming exercises, implementing a holistic security system that offers in-depth visibility and total control over attack surfaces, and improving attack readiness, organizations can ensure preparedness for both current and future challenges in this new era of AI-powered cybercrime.\n\t\t\t\nMalware creation is just a starting point of what amateur threat actors will achieve with AI. In the not-so-distant future, novices will be able to design sophisticated social engineering campaigns, analyze target environments, identify vulnerabilities, choose attack vectors, orchestrate multi-stage attacks, automate target selection and attack execution, etc.  AI bots will monitor their own operations and adapt their tactics based on what they learn about the target entity or environment.\n\n\n\nIn a nutshell, the volume of low-skilled threat actors and their expertise is all set to grow exponentially, thanks to AI.\n\n\n\nWhat Can Organizations Do To Fight Back?\nThe rise of the zero-knowledge threat actor should be a wakeup call for organizations—cyberattacks are only going to be smarter, better and more frequent. Below are some recommendations and best practices that can help:\n\n\n\n\nIncrease Employee Awareness: Conduct training sessions to inform employees about the growing risks of AI-powered threat actors. Run simulated AI attacks and fire drills to improve alertness and vigilance among employees.\nConduct AI Red Teaming: If your organization uses or creates AI tools, test these systems against malicious prompts and validate if they are vulnerable to jailbreaking. Invest time and resources to anticipate AI attacks and stress-test systems against those scenarios.\nImplement Holistic Security: Deploy an end-to-end security system like SASE, not fragmented tools, to monitor, detect and analyze malicious signals across the entire IT infrastructure (users, clouds, devices, networks).\nPatch Systems and Software Frequently: Ensure that tools and the software you use in your organization are the latest and greatest versions. If you don’t fix loopholes, AI-powered adversaries will certainly seek them out and exploit them.\nImprove Attack Readiness: A proactive and well-practiced incident response plan will not only minimize damage but also strengthen organizational resilience against the unpredictability of AI-powered threats.\nAdopt Security Frameworks: Follow best practices as advocated by stalwart standard-bearers such as MITRE ATLAS, OWASP Top 10 for LLM Applications, and Google’s Secure AI Framework (SAIF).\n\nThe rise of zero-knowledge threat actors powered by AI marks a turning point in the business of cybercrime where sophisticated attacks are no longer confined to skilled attackers. By conducting red teaming exercises, implementing a holistic security system that offers in-depth visibility and total control over attack surfaces, and improving attack readiness, organizations can ensure preparedness for both current and future challenges in this new era of AI-powered cybercrime.\n\t\t\t\nIn a nutshell, the volume of low-skilled threat actors and their expertise is all set to grow exponentially, thanks to AI.\n\n\n\nWhat Can Organizations Do To Fight Back?\nThe rise of the zero-knowledge threat actor should be a wakeup call for organizations—cyberattacks are only going to be smarter, better and more frequent. Below are some recommendations and best practices that can help:\n\n\n\n\nIncrease Employee Awareness: Conduct training sessions to inform employees about the growing risks of AI-powered threat actors. Run simulated AI attacks and fire drills to improve alertness and vigilance among employees.\nConduct AI Red Teaming: If your organization uses or creates AI tools, test these systems against malicious prompts and validate if they are vulnerable to jailbreaking. Invest time and resources to anticipate AI attacks and stress-test systems against those scenarios.\nImplement Holistic Security: Deploy an end-to-end security system like SASE, not fragmented tools, to monitor, detect and analyze malicious signals across the entire IT infrastructure (users, clouds, devices, networks).\nPatch Systems and Software Frequently: Ensure that tools and the software you use in your organization are the latest and greatest versions. If you don’t fix loopholes, AI-powered adversaries will certainly seek them out and exploit them.\nImprove Attack Readiness: A proactive and well-practiced incident response plan will not only minimize damage but also strengthen organizational resilience against the unpredictability of AI-powered threats.\nAdopt Security Frameworks: Follow best practices as advocated by stalwart standard-bearers such as MITRE ATLAS, OWASP Top 10 for LLM Applications, and Google’s Secure AI Framework (SAIF).\n\nThe rise of zero-knowledge threat actors powered by AI marks a turning point in the business of cybercrime where sophisticated attacks are no longer confined to skilled attackers. By conducting red teaming exercises, implementing a holistic security system that offers in-depth visibility and total control over attack surfaces, and improving attack readiness, organizations can ensure preparedness for both current and future challenges in this new era of AI-powered cybercrime.\n\t\t\t\nWhat Can Organizations Do To Fight Back?\nThe rise of the zero-knowledge threat actor should be a wakeup call for organizations—cyberattacks are only going to be smarter, better and more frequent. Below are some recommendations and best practices that can help:\n\n\n\n\nIncrease Employee Awareness: Conduct training sessions to inform employees about the growing risks of AI-powered threat actors. Run simulated AI attacks and fire drills to improve alertness and vigilance among employees.\nConduct AI Red Teaming: If your organization uses or creates AI tools, test these systems against malicious prompts and validate if they are vulnerable to jailbreaking. Invest time and resources to anticipate AI attacks and stress-test systems against those scenarios.\nImplement Holistic Security: Deploy an end-to-end security system like SASE, not fragmented tools, to monitor, detect and analyze malicious signals across the entire IT infrastructure (users, clouds, devices, networks).\nPatch Systems and Software Frequently: Ensure that tools and the software you use in your organization are the latest and greatest versions. If you don’t fix loopholes, AI-powered adversaries will certainly seek them out and exploit them.\nImprove Attack Readiness: A proactive and well-practiced incident response plan will not only minimize damage but also strengthen organizational resilience against the unpredictability of AI-powered threats.\nAdopt Security Frameworks: Follow best practices as advocated by stalwart standard-bearers such as MITRE ATLAS, OWASP Top 10 for LLM Applications, and Google’s Secure AI Framework (SAIF).\n\nThe rise of zero-knowledge threat actors powered by AI marks a turning point in the business of cybercrime where sophisticated attacks are no longer confined to skilled attackers. By conducting red teaming exercises, implementing a holistic security system that offers in-depth visibility and total control over attack surfaces, and improving attack readiness, organizations can ensure preparedness for both current and future challenges in this new era of AI-powered cybercrime.\n\t\t\t\nThe rise of the zero-knowledge threat actor should be a wakeup call for organizations—cyberattacks are only going to be smarter, better and more frequent. Below are some recommendations and best practices that can help:\n\n\n\n\nIncrease Employee Awareness: Conduct training sessions to inform employees about the growing risks of AI-powered threat actors. Run simulated AI attacks and fire drills to improve alertness and vigilance among employees.\nConduct AI Red Teaming: If your organization uses or creates AI tools, test these systems against malicious prompts and validate if they are vulnerable to jailbreaking. Invest time and resources to anticipate AI attacks and stress-test systems against those scenarios.\nImplement Holistic Security: Deploy an end-to-end security system like SASE, not fragmented tools, to monitor, detect and analyze malicious signals across the entire IT infrastructure (users, clouds, devices, networks).\nPatch Systems and Software Frequently: Ensure that tools and the software you use in your organization are the latest and greatest versions. If you don’t fix loopholes, AI-powered adversaries will certainly seek them out and exploit them.\nImprove Attack Readiness: A proactive and well-practiced incident response plan will not only minimize damage but also strengthen organizational resilience against the unpredictability of AI-powered threats.\nAdopt Security Frameworks: Follow best practices as advocated by stalwart standard-bearers such as MITRE ATLAS, OWASP Top 10 for LLM Applications, and Google’s Secure AI Framework (SAIF).\n\nThe rise of zero-knowledge threat actors powered by AI marks a turning point in the business of cybercrime where sophisticated attacks are no longer confined to skilled attackers. By conducting red teaming exercises, implementing a holistic security system that offers in-depth visibility and total control over attack surfaces, and improving attack readiness, organizations can ensure preparedness for both current and future challenges in this new era of AI-powered cybercrime.\n\t\t\t\nThe rise of zero-knowledge threat actors powered by AI marks a turning point in the business of cybercrime where sophisticated attacks are no longer confined to skilled attackers. By conducting red teaming exercises, implementing a holistic security system that offers in-depth visibility and total control over attack surfaces, and improving attack readiness, organizations can ensure preparedness for both current and future challenges in this new era of AI-powered cybercrime.\n\t\t\t\nEtay Maor is Chief Security Strategist and founding member of Cyber Threats Research Lab (CTRL) at Cato Networks. Previously, he was Chief Security Officer for IntSights and held senior security positions at IBM and RSA Security's Cyber Threats Research Labs. An adjunct professor at Boston College, he holds a BA in computer science and a MA in counterterrorism and cyber terrorism from Reichman University (IDC Herzliya), Tel Aviv.\nSubscribe to the SecurityWeek Email Briefing to stay informed on the latest threats, trends, and technology, along with insightful columns from industry experts.\nJoin this event as we dive into threat hunting tools and frameworks, and explore value of threat intelligence data in the defender’s security stack.\nLearn how integrating BAS and Automated Penetration Testing empowers security teams to quickly identify and validate threats, enabling prompt response and remediation.\nIdentity verification and fraud prevention firm Trulioo has appointed Vicky Bindra as CEO.\nCommvault has appointed Bill O'Connell as its Chief Security Officer.\nKevin Paige has been appointed Field CISO at identity governance company ConductorOne.\n\n\nA strong security program will sometimes require substantial organizational and cultural changes around security practices, and inevitably, a higher cost.\n(Trevin Edgeworth)\n\n\n\nInterview with Taylor Pyle, a Cybersecurity Engineer at Viasat on her experience with both cyber and mentorship.\n(Marc Solomon)\n\n\n\nA Joni Mitchell song from the 1960s can teach us a lot about securing hybrid and multi-cloud environments.\n(Joshua Goldfarb)\n\n\n\nDefending high profile sporting events from adversarial attacks requires a mix of experienced capabilities and a solid threat intelligence program.\n(Marc Solomon)\n\n\n\nYour guide on how to get through the conference with your sanity, energy, and key performance indicators (KPIs) intact.\n(Jennifer Leggio)\n\nFlipboard\nReddit\nWhatsapp\nWhatsapp\nEmail\nGot a confidential news tip? We want to hear from you.\nReach a large audience of enterprise cybersecurity professionals \nSubscribe to the SecurityWeek Daily Briefing and get the latest content delivered to your inbox.\nCopyright © 2025 SecurityWeek ®, a Wired Business Media Publication. All Rights Reserved. \n\r\n\t\t\t\t\tSubscribe to the SecurityWeek Email Briefing to stay informed on the latest cybersecurity\r\n\t\t\t\t\tnews, threats, and expert insights. Unsubscribe at any time.\r\n\t\t\t\t\t\n\n\n"
    },
    {
        "Title": "Romanian Security Firm Allegedly Breached Affecting 586K Customers",
        "Link": "https://dailydarkweb.net/romanian-security-firm-allegedly-breached-affecting-586k-customers/",
        "Summary": "<p>A threat actor has allegedly put up for sale what they claim to be a database belonging to Atutech, a Romanian company specializing in the import, distribution, and installation of security systems. The post, published on a dark web forum, allegedly contains sensitive customer information, including names, email addresses, billing and shipping addresses, and other [&#8230;]</p>\n<p>The post <a href=\"https://dailydarkweb.net/romanian-security-firm-allegedly-breached-affecting-586k-customers/\">Romanian Security Firm Allegedly Breached Affecting 586K Customers</a> appeared first on <a href=\"https://dailydarkweb.net\">Daily Dark Web</a>.</p>",
        "Date": "2025-04-02",
        "Content": "A threat actor has allegedly put up for sale what they claim to be a database belonging to Atutech, a Romanian company specializing in the import, distribution, and installation of security systems. The post, published on a dark web forum, allegedly contains sensitive customer information, including names, email addresses, billing and shipping addresses, and other order details.\nAccording to the threat actor, the alleged breach occurred in 2025, affecting approximately 586,213 customers. They claim that the compromised data includes full names, email addresses, physical addresses, and even invoice and warranty documents linked to customer purchases. Additionally, the post suggests that the leaked information contains details of transactions.\nThe actor is reportedly offering this dataset for sale to interested buyers on the dark web. The seller has provided sample records, which they allege are proof of their access to Atutech’s internal systems.\nTags:\ncorporate data\ncustomer-data\ndata-breach\ndata-leak\nPonce\nRomania\nsecurity\nsensitive-data"
    },
    {
        "Title": "Alleged Access to Mexican Territorial Operating System Exposes Thousands of Records",
        "Link": "https://dailydarkweb.net/alleged-access-to-mexican-territorial-operating-system-exposes-thousands-of-records/",
        "Summary": "<p>In post on a dark web forum, a threat actor has allegedly put up for sale access to what they claim is the &#8220;Mexican territorial operating system.&#8221; According to the post, the hacker is offering access along with a trove of extracted data for a price of $2,000 USD. The individual behind the claim has [&#8230;]</p>\n<p>The post <a href=\"https://dailydarkweb.net/alleged-access-to-mexican-territorial-operating-system-exposes-thousands-of-records/\">Alleged Access to Mexican Territorial Operating System Exposes Thousands of Records</a> appeared first on <a href=\"https://dailydarkweb.net\">Daily Dark Web</a>.</p>",
        "Date": "2025-04-02",
        "Content": "In post on a dark web forum, a threat actor has allegedly put up for sale access to what they claim is the “Mexican territorial operating system.” According to the post, the hacker is offering access along with a trove of extracted data for a price of $2,000 USD.\nThe individual behind the claim has purportedly listed various types of sensitive data, including user tickets, passwords, tokens, WhatsApp messages, text messages, and voting information.\nThe post, which was addressed to a hacking community, also included an external link purportedly containing images that supposedly demonstrate the extent of the data breach. The threat actor emphasized that only serious buyers should inquire about the sale.\nTags:\naccess\nAKA_Astaroth\ndata-breach\ngovernment\ngovernment data\ngovernment platform\ngovernment systems\nMexico\nPersonal Data"
    },
    {
        "Title": "Alleged Data Breach of Thailand Post Exposes 19M Records",
        "Link": "https://dailydarkweb.net/alleged-data-breach-of-thailand-post-exposes-19m-records/",
        "Summary": "<p>A threat actor has allegedly posted a claim on a dark web forum, stating that they have obtained and are selling a database belonging to Thailand Post. The post allegedly contains sensitive customer data, including names, phone numbers, email addresses, transaction records, and other personally identifiable information. According to the forum post the database contains [&#8230;]</p>\n<p>The post <a href=\"https://dailydarkweb.net/alleged-data-breach-of-thailand-post-exposes-19m-records/\">Alleged Data Breach of Thailand Post Exposes 19M Records</a> appeared first on <a href=\"https://dailydarkweb.net\">Daily Dark Web</a>.</p>",
        "Date": "2025-04-02",
        "Content": "A threat actor has allegedly posted a claim on a dark web forum, stating that they have obtained and are selling a database belonging to Thailand Post. The post allegedly contains sensitive customer data, including names, phone numbers, email addresses, transaction records, and other personally identifiable information.\nAccording to the forum post the database contains approximately 19 million records. The seller has provided a sample of the data, though they claim that many columns contain null values. The forum post states that while the dataset may not be fully populated with all information fields, it still holds a significant amount of customer data that could be exploited by malicious actors.\nThe threat actor has allegedly listed the stolen database for sale in CSV format and has claimed they are willing to provide the full dataset after a transaction is completed. They have also mentioned that the dataset includes information such as birth dates, addresses, membership details, and transaction histories.\nTags:\ndata-breach\ngovernment\ngovernment data\ngovernment platform\nLogistics\npost\npostal service\nThailand\nvebxpert"
    }
]